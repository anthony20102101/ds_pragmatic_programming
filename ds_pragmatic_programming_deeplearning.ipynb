{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "1. ~Ho to define models~ \n",
    "\n",
    "    * ~Helow word simple regression~\n",
    "    * ~define models with sequence~\n",
    "    \n",
    "1. Callbacks \n",
    "    * How to stop when reach specific acc or perfromance \n",
    "    * How to save best model during \n",
    "    * How to save models archicterue, parameters, optmizer parameters (everything) (see my code in dialects)\n",
    "    * How to load saved models \n",
    "\n",
    "1. ~Vectorization operations and loss functions~\n",
    "\n",
    "    * ~create ones and zeros and get shape~\n",
    "    * ~matrix n vector mutliplications~\n",
    "    * ~reshape matrix~\n",
    "    * ~How to encode one-hot~\n",
    "    \n",
    "1. Code for main loss functions and when to use\n",
    "\n",
    "    * binnary cross entropy\n",
    "    * categorical cross entropy\n",
    "    * sparse categorical cross entropy\n",
    "    * mae\n",
    "    * mse \n",
    "\n",
    "1. Learning curves \n",
    "    * Code to plot learning curves in notebook using matplotlib\n",
    "    * Code to use tensorboard. Add how to use and interpret the histogram of weight intensorboard\n",
    "\n",
    "1. Code to use Genrators\n",
    "    * Split folders in train and val so Generators can read \n",
    "    * ImageDataGenerator with data augmentation\n",
    "    * Use ImageDataGenerator to save aigmented data\n",
    "\n",
    "refs:\n",
    "* https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:21:22.344578Z",
     "start_time": "2019-05-16T01:21:11.912985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "#slim = tf.contrib.slim\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello word (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking\n",
      "x: 3.0; expected: 5.0; prediction: [[5.000253]]\n",
      "unseen data\n",
      "x: 10.0; expected: 19.0; prediction: [[18.982594]]\n"
     ]
    }
   ],
   "source": [
    "# the data\n",
    "# rule that We need to find is y = 2x - 1\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0,1.0,3.0,5.0,7.0], dtype=float)\n",
    "\n",
    "# Define a model with one neuron\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "# define loss func and optimizer \n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# train or learn the rule\n",
    "model.fit(xs, ys, epochs=500, verbose=0)\n",
    "\n",
    "print(\"Checking\")\n",
    "x = 3.0\n",
    "print(f\"x: {x}; expected: {2.0*x-1}; prediction: {model.predict([x])}\")\n",
    "\n",
    "print(\"unseen data\")\n",
    "x = 10.00\n",
    "print(f\"x: {x}; expected: {2.0*x-1}; prediction: {model.predict([x])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor basic operations \n",
    "\n",
    "https://www.tensorflow.org/guide/tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector add: [4 6]\n",
      "sum all elemenst: 6\n",
      "mymat: [[ 7]\n",
      " [11]]\n",
      "batch_imgs: (10, 299, 299, 3)\n",
      "\n",
      "loss: 9\n",
      "\n",
      "numpy compatibility\n",
      "Tensor convert to numpy\n",
      "tensor: [[10. 10. 10.]\n",
      " [10. 10. 10.]\n",
      " [10. 10. 10.]]\n",
      "\n",
      "And Tensors converts to numpy\n",
      " add scalr to tensor and get a numpy array:  [[11. 11. 11.]\n",
      " [11. 11. 11.]\n",
      " [11. 11. 11.]]\n",
      "\n",
      "The .numpy() method explicitly converts a Tensor to a numpy array\n",
      "[[10. 10. 10.]\n",
      " [10. 10. 10.]\n",
      " [10. 10. 10.]]\n",
      "\n",
      "tensor ones: <tf.Variable 'Variable:0' shape=(3, 3) dtype=float64, numpy=\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.]])>\n",
      "\n",
      "Reshaping\n",
      "x shape: (3, 3)\n",
      "x: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "y shape: (2, 6)\n",
      "x: [[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n",
      "\n",
      "Transpose\n",
      "y shape: (2, 6)\n",
      "y.T: (6, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"vector add: {tf.add([1, 2], [3, 4])}\")\n",
    "print(f\"sum all elemenst: {tf.reduce_sum([1, 2, 3])}\")\n",
    "\n",
    "mymat = tf.Variable([[7],[11]], tf.int16)\n",
    "print(f\"mymat: {mymat.value()}\")\n",
    "\n",
    "# batch x height x width x color\n",
    "batch_imgs = tf.zeros([10, 299, 299, 3])  \n",
    "print(f\"batch_imgs: {batch_imgs.shape}\")\n",
    "\n",
    "print()\n",
    "\n",
    "y_hat = tf.constant(36)            # Define y_hat constant. Set to 36.\n",
    "y = tf.constant(39)   \n",
    "\n",
    "loss = tf.Variable((y - y_hat)**2)\n",
    "\n",
    "print(f\"loss: {loss.value()}\")\n",
    "\n",
    "print()\n",
    "print(\"numpy compatibility\")\n",
    "\n",
    "ndarray = np.ones([3, 3])\n",
    "\n",
    "print(\"Tensor convert to numpy\")\n",
    "tensor = tf.multiply(ndarray, 10)\n",
    "print(f\"tensor: {tensor}\")\n",
    "\n",
    "print()\n",
    "print(\"And Tensors converts to numpy\")\n",
    "print(f\" add scalr to tensor and get a numpy array:  {np.add(tensor, 1)}\")\n",
    "\n",
    "print()\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())\n",
    "\n",
    "print()\n",
    "print(f\"tensor ones: {tf.Variable(np.ones([3, 3]))}\")\n",
    "\n",
    "print()\n",
    "print(\"Reshaping\")\n",
    "\n",
    "x = tf.reshape(np.array([1,2,3,4,5,6,7,8,9]), [3, 3])\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"x: {x.numpy()}\")\n",
    "\n",
    "# -1 means infere this dimension\n",
    "y = tf.reshape(np.array([1,2,3,4,5,6,7,8,9,10,11,12]), [2, -1])\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"x: {y.numpy()}\")\n",
    "\n",
    "print()\n",
    "print(\"Transpose\")\n",
    "# Transpose\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y.T: {tf.transpose(y).shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers: Understanding, Debbuging  and analyze the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 1,  2],\n",
       "        [ 2,  3],\n",
       "        [ 3,  4],\n",
       "        [ 4,  5],\n",
       "        [ 5,  6],\n",
       "        [ 6,  7],\n",
       "        [ 7,  8],\n",
       "        [ 8,  9],\n",
       "        [ 9, 10]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([range(10),range(10)])\n",
    "X[1,:] = X[1,:] + 1 \n",
    "X = np.reshape(X,(2,10,1))\n",
    "X.shape\n",
    "X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GlobalAveragePooling1D and MaxPool1D\n",
    "\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D\n",
    "\n",
    "```python\n",
    "tf.keras.layers.MaxPool1D(\n",
    "    pool_size=2, strides=None, padding='valid', data_format='channels_last',\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\n",
    "# filters is the number of neurons\n",
    "tf.keras.layers.Conv1D(\n",
    "    filters, kernel_size, strides=1, padding='valid', data_format='channels_last',\n",
    "    dilation_rate=1, activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 8, 1)              4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 8, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 1,  2],\n",
       "        [ 2,  3],\n",
       "        [ 3,  4],\n",
       "        [ 4,  5],\n",
       "        [ 5,  6],\n",
       "        [ 6,  7],\n",
       "        [ 7,  8],\n",
       "        [ 8,  9],\n",
       "        [ 9, 10]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Conv1d\")\n",
    "\n",
    "_kernel_init_coefs = np.array([-1, 0, 1])\n",
    "_kernel_init_coefs = tf.initializers.constant(_kernel_init_coefs)\n",
    "\n",
    "\n",
    "model3 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(1, kernel_size=3, kernel_initializer= _kernel_init_coefs, input_shape=(10,1) )\n",
    "    ])\n",
    "\n",
    "model3.summary()\n",
    "y3 = model3.predict(X)\n",
    "y3.shape\n",
    "\n",
    "X.T\n",
    "\n",
    "# Expected all 2 values becasue: \n",
    "# 0*-1 + 1*0 + 2*1 = 2\n",
    "# 1*-1 + 2*0 + 3*1 = 2\n",
    "# ... \n",
    "y3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalAveragePooling1D\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling1d_5 ( (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[4.5, 5.5]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MaxPool1D\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 5, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.],\n",
       "        [ 3.,  4.],\n",
       "        [ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"GlobalAveragePooling1D\")\n",
    "model1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.GlobalAveragePooling1D(input_shape=(10,1) )\n",
    "    ])\n",
    "model1.summary()\n",
    "y1 = model1.predict(X)\n",
    "y1.shape\n",
    "y1.T\n",
    "\n",
    "print()\n",
    "print(\"MaxPool1D\")\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.MaxPool1D(pool_size=2, input_shape=(10,1) )\n",
    "    ])\n",
    "\n",
    "model2.summary()\n",
    "y2 = model2.predict(X)\n",
    "y2.shape\n",
    "\n",
    "y2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_11 (Conv1D)           (None, 8, 1)              4         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 4, 1)              0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Layer: 0\n",
      "output shape: (2, 8, 1)\n",
      "[[[ 3.  6.]\n",
      "  [ 6.  9.]\n",
      "  [ 9. 12.]\n",
      "  [12. 15.]\n",
      "  [15. 18.]\n",
      "  [18. 21.]\n",
      "  [21. 24.]\n",
      "  [24. 27.]]]\n",
      "Layer: 1\n",
      "output shape: (2, 4, 1)\n",
      "[[[ 6.  9.]\n",
      "  [12. 15.]\n",
      "  [18. 21.]\n",
      "  [24. 27.]]]\n",
      "Layer: 2\n",
      "output shape: (2, 1)\n",
      "[[15. 18.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "_kernel_init_coefs = np.array([1, 1, 1])\n",
    "_kernel_init_coefs = tf.initializers.constant(_kernel_init_coefs)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(1, kernel_size=3, kernel_initializer= _kernel_init_coefs, input_shape=(10,1) ),\n",
    "        tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "        tf.keras.layers.GlobalAveragePooling1D()\n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "\n",
    "for l in range(3):\n",
    "    \n",
    "    print(f\"Layer: {l}\")\n",
    "    \n",
    "    f1 = activation_model.predict(X)[l]\n",
    "    print(f\"output shape: {f1.shape}\")\n",
    "    print(f1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:16:21.750111Z",
     "start_time": "2019-05-14T23:16:21.746973Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Encoding\n",
    "\n",
    "* One hot encoding\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "\n",
    "<img src=\"images/onehot.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: 2; one-hot encode: [0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "test_labels = tf.constant([1,2, 3, 0, 2, 1])\n",
    "\n",
    "y = to_categorical(test_labels)\n",
    "y.shape\n",
    "\n",
    "k = 1\n",
    "print(f\"labels: {test_labels[k]}; one-hot encode: {y[k,:]}\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=825, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [0, 1, 2]\n",
    "depth = 3\n",
    "\n",
    "# ?tf.one_hot\n",
    "tf.one_hot(indices, depth)  # output: [3 x 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train \n",
    "\n",
    "\n",
    "- **Training set**: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).\n",
    "- **Test set**: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:20:05.450391Z",
     "start_time": "2019-05-15T22:20:05.405403Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    train_dataset = h5py.File('data/train_signs.h5', \"r\")\n",
    "    \n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('data/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:20:06.228960Z",
     "start_time": "2019-05-15T22:20:06.076697Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.shape(classes))\n",
    "print(classes)\n",
    "\n",
    "# See example of image\n",
    "index = 0\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* preprocessing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:48:24.233605Z",
     "start_time": "2019-05-14T23:48:24.164274Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten / 255.\n",
    "X_test = X_test_flatten / 255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print(\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[1]))\n",
    "\n",
    "print()\n",
    "print('64*64*3 = 12288')\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "print()\n",
    "print (\"Y_test (1st 5) = \\n\" + str(np.squeeze(Y_test[:,0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and metrics \n",
    "\n",
    "ref: https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE and MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.0625\n",
      "mae: 0.125\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([1  ,0, 1, 0])\n",
    "y_pred = np.array([0.5,0, 1, 0])\n",
    "\n",
    "mse = tf.keras.losses.MSE(y_true, y_pred).numpy()\n",
    "mae = tf.keras.losses.MAE(y_true, y_pred).numpy()\n",
    "\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"mae: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "```python\n",
    "\n",
    "# binary \n",
    "tf.keras.losses.binary_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, label_smoothing=0\n",
    ")\n",
    "```\n",
    "\n",
    "* binary cross entropy\n",
    "\n",
    "$\n",
    "- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log (\\hat{y}^{(i)}) + (1-y^{(i)})\\log (1-\\hat{y}^{(i)} \\large ) \\small\\tag{1}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3273332118988037\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([1,0, 1, 0])\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = tf.constant([0.3,0, 0.9, 0])\n",
    "\n",
    "_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "# categorical crossentropy\n",
    "tf.keras.losses.categorical_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, label_smoothing=0\n",
    ")\n",
    "\n",
    "# sparse\n",
    "tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, axis=-1\n",
    ")\n",
    "\n",
    "# Use the loss function (approx. 1 line)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [1.1920929e-07 1.1920929e-07 1.1920929e-07 1.1920929e-07]\n"
     ]
    }
   ],
   "source": [
    "## TODO needs to be reviewed \n",
    "y_true = to_categorical(tf.constant([1,0, 1, 0]))\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = to_categorical(tf.constant([0.3,0, 0.9, 0]))\n",
    "\n",
    "_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1164, shape=(4, 3), dtype=float32, numpy=\n",
       "array([[0. , 0.9, 0.1],\n",
       "       [0. , 0.3, 0.7],\n",
       "       [0.5, 0.9, 0.5],\n",
       "       [0.7, 0.1, 0.2]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [4] vs. [4,3] [Op:Mul] name: mul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-bf8d2ff87fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0m_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loss: {_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dialects/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing)\u001b[0m\n\u001b[1;32m    969\u001b[0m   y_true = smart_cond.smart_cond(label_smoothing,\n\u001b[1;32m    970\u001b[0m                                  _smooth_labels, lambda: y_true)\n\u001b[0;32m--> 971\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dialects/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   4466\u001b[0m       \u001b[0mepsilon_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4467\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4468\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4470\u001b[0m       \u001b[0;31m# When softmax activation function is used for output operation, we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dialects/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dialects/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1204\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dialects/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6696\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6697\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6698\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6699\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6700\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
      "\u001b[0;32m~/anaconda3/envs/dialects/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [4] vs. [4,3] [Op:Mul] name: mul/"
     ]
    }
   ],
   "source": [
    "## TODO needs to be reviewed \n",
    "y_true = tf.constant([1,2, 1, 0])\n",
    "\n",
    "\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = tf.constant([ [0.0,0.9, 0.1],\n",
    "                       [0.0,0.3, 0.7],\n",
    "                       [0.5,0.9, 0.5],\n",
    "                       [0.7,0.1, 0.2]\n",
    "                     ])\n",
    "\n",
    "y_pred.shape\n",
    "y_pred\n",
    "\n",
    "_loss = tf.keras.losses.categorical_crossentropy(y_true,y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* sigmoid_cross_entropy_with_logits\n",
    "\n",
    "```python\n",
    "\n",
    "# Use the loss function (approx. 1 line)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "\n",
    "```\n",
    "\n",
    "$\n",
    "- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Convolutional (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:22:57.512166Z",
     "start_time": "2019-05-15T03:22:57.488726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datasets import flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialects",
   "language": "python",
   "name": "dialects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
