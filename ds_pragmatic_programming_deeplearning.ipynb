{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "\n",
    "**TODO**\n",
    "\n",
    "1. ~Ho to define models~ \n",
    "\n",
    "    * ~Helow word simple regression~\n",
    "    * ~define models with sequence~\n",
    "    \n",
    "1. Callbacks \n",
    "    * How to stop when reach specific acc or perfromance \n",
    "    * How to save best model during \n",
    "    * How to save models archicterue, parameters, optmizer parameters (everything) (see my code in dialects)\n",
    "    * How to load saved models \n",
    "\n",
    "1. ~Vectorization operations and loss functions~\n",
    "\n",
    "    * ~create ones and zeros and get shape~\n",
    "    * ~matrix n vector mutliplications~\n",
    "    * ~reshape matrix~\n",
    "    * ~How to encode one-hot~\n",
    "    \n",
    "1. Code for main loss functions and when to use\n",
    "\n",
    "    * binnary cross entropy\n",
    "    * categorical cross entropy\n",
    "    * sparse categorical cross entropy\n",
    "    * mae\n",
    "    * mse \n",
    "\n",
    "1. Learning curves \n",
    "    * Code to plot learning curves in notebook using matplotlib\n",
    "    * Code to use tensorboard. Add how to use and interpret the histogram of weight intensorboard\n",
    "\n",
    "1. Code to use Genrators\n",
    "    * Split folders in train and val so Generators can read \n",
    "    * ImageDataGenerator with data augmentation\n",
    "    * Use ImageDataGenerator to save aigmented data\n",
    "\n",
    "refs:\n",
    "* https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:21:22.344578Z",
     "start_time": "2019-05-16T01:21:11.912985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "#slim = tf.contrib.slim\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello word (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking\n",
      "x: 3.0; expected: 5.0; prediction: [[5.000253]]\n",
      "unseen data\n",
      "x: 10.0; expected: 19.0; prediction: [[18.982594]]\n"
     ]
    }
   ],
   "source": [
    "# the data\n",
    "# rule that We need to find is y = 2x - 1\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0,1.0,3.0,5.0,7.0], dtype=float)\n",
    "\n",
    "# Define a model with one neuron\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "# define loss func and optimizer \n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# train or learn the rule\n",
    "model.fit(xs, ys, epochs=500, verbose=0)\n",
    "\n",
    "print(\"Checking\")\n",
    "x = 3.0\n",
    "print(f\"x: {x}; expected: {2.0*x-1}; prediction: {model.predict([x])}\")\n",
    "\n",
    "print(\"unseen data\")\n",
    "x = 10.00\n",
    "print(f\"x: {x}; expected: {2.0*x-1}; prediction: {model.predict([x])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor basic operations \n",
    "\n",
    "https://www.tensorflow.org/guide/tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector add: [4 6]\n",
      "sum all elemenst: 6\n",
      "mymat: [[ 7]\n",
      " [11]]\n",
      "batch_imgs: (10, 299, 299, 3)\n",
      "\n",
      "loss: 9\n",
      "\n",
      "numpy compatibility\n",
      "Tensor convert to numpy\n",
      "tensor: [[10. 10. 10.]\n",
      " [10. 10. 10.]\n",
      " [10. 10. 10.]]\n",
      "\n",
      "And Tensors converts to numpy\n",
      " add scalr to tensor and get a numpy array:  [[11. 11. 11.]\n",
      " [11. 11. 11.]\n",
      " [11. 11. 11.]]\n",
      "\n",
      "The .numpy() method explicitly converts a Tensor to a numpy array\n",
      "[[10. 10. 10.]\n",
      " [10. 10. 10.]\n",
      " [10. 10. 10.]]\n",
      "\n",
      "tensor ones: <tf.Variable 'Variable:0' shape=(3, 3) dtype=float64, numpy=\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.]])>\n",
      "\n",
      "Reshaping\n",
      "x shape: (3, 3)\n",
      "x: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "y shape: (2, 6)\n",
      "x: [[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"vector add: {tf.add([1, 2], [3, 4])}\")\n",
    "print(f\"sum all elemenst: {tf.reduce_sum([1, 2, 3])}\")\n",
    "\n",
    "mymat = tf.Variable([[7],[11]], tf.int16)\n",
    "print(f\"mymat: {mymat.value()}\")\n",
    "\n",
    "# batch x height x width x color\n",
    "batch_imgs = tf.zeros([10, 299, 299, 3])  \n",
    "print(f\"batch_imgs: {batch_imgs.shape}\")\n",
    "\n",
    "print()\n",
    "\n",
    "y_hat = tf.constant(36)            # Define y_hat constant. Set to 36.\n",
    "y = tf.constant(39)   \n",
    "\n",
    "loss = tf.Variable((y - y_hat)**2)\n",
    "\n",
    "print(f\"loss: {loss.value()}\")\n",
    "\n",
    "print()\n",
    "print(\"numpy compatibility\")\n",
    "\n",
    "ndarray = np.ones([3, 3])\n",
    "\n",
    "print(\"Tensor convert to numpy\")\n",
    "tensor = tf.multiply(ndarray, 10)\n",
    "print(f\"tensor: {tensor}\")\n",
    "\n",
    "print()\n",
    "print(\"And Tensors converts to numpy\")\n",
    "print(f\" add scalr to tensor and get a numpy array:  {np.add(tensor, 1)}\")\n",
    "\n",
    "print()\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())\n",
    "\n",
    "print()\n",
    "print(f\"tensor ones: {tf.Variable(np.ones([3, 3]))}\")\n",
    "\n",
    "print()\n",
    "print(\"Reshaping\")\n",
    "\n",
    "x = tf.reshape(np.array([1,2,3,4,5,6,7,8,9]), [3, 3])\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"x: {x.numpy()}\")\n",
    "\n",
    "# -1 means infere this dimension\n",
    "y = tf.reshape(np.array([1,2,3,4,5,6,7,8,9,10,11,12]), [2, -1])\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"x: {y.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:16:21.750111Z",
     "start_time": "2019-05-14T23:16:21.746973Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Encoding\n",
    "\n",
    "* One hot encoding\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "\n",
    "<img src=\"images/onehot.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: 2; one-hot encode: [0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "test_labels = tf.constant([1,2, 3, 0, 2, 1])\n",
    "\n",
    "y = to_categorical(test_labels)\n",
    "y.shape\n",
    "\n",
    "k = 1\n",
    "print(f\"labels: {test_labels[k]}; one-hot encode: {y[k,:]}\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=825, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [0, 1, 2]\n",
    "depth = 3\n",
    "\n",
    "# ?tf.one_hot\n",
    "tf.one_hot(indices, depth)  # output: [3 x 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train \n",
    "\n",
    "\n",
    "- **Training set**: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).\n",
    "- **Test set**: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:20:05.450391Z",
     "start_time": "2019-05-15T22:20:05.405403Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    train_dataset = h5py.File('data/train_signs.h5', \"r\")\n",
    "    \n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('data/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:20:06.228960Z",
     "start_time": "2019-05-15T22:20:06.076697Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.shape(classes))\n",
    "print(classes)\n",
    "\n",
    "# See example of image\n",
    "index = 0\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* preprocessing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:48:24.233605Z",
     "start_time": "2019-05-14T23:48:24.164274Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten / 255.\n",
    "X_test = X_test_flatten / 255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print(\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[1]))\n",
    "\n",
    "print()\n",
    "print('64*64*3 = 12288')\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "print()\n",
    "print (\"Y_test (1st 5) = \\n\" + str(np.squeeze(Y_test[:,0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Use the loss function (approx. 1 line)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "\n",
    "```\n",
    "\n",
    "$$- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_network():\n",
    "    \n",
    "    with tf.variable_scope('mymodel'):\n",
    "        \n",
    "    # Add a fully connected layer with 100 units.\n",
    "    num_units = 100\n",
    "    fc = slim.fully_connected(embeddings, num_units)\n",
    "\n",
    "    # Add a classifier layer at the end, consisting of parallel logistic\n",
    "    # classifiers, one per class. This allows for multi-class tasks.\n",
    "    logits = slim.fully_connected( fc, _NUM_CLASSES, activation_fn=None, scope='logits')\n",
    "        \n",
    "    tf.sigmoid(logits, name='prediction')\n",
    "\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction or inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow slim\n",
    "\n",
    "**TF-Slim: A high level library to define complex models in TensorFlow**\n",
    "\n",
    "refs:\n",
    "\n",
    "https://cv-tricks.com/tensorflow-tutorial/understanding-alexnet-resnet-squeezenetand-running-on-tensorflow/\n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/slim/slim_walkthrough.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Comparing slim with tensorflow. You need less lines of code to do the same**. But tensorflow gives you more flexibility and it is easy to see the learning curves\n",
    "\n",
    "```python\n",
    "# using slim\n",
    "net = slim.conv2d(input, 128,[3, 3], scope='conv1_1')\n",
    "\n",
    "# slim version\n",
    "with tf.name_scope('conv1_1') as scope:\n",
    "    \n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3,  64,128], dtype=tf.float32,stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(input, kernel,[1, 1, 1, 1], padding='SAME')\n",
    " \n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases')\n",
    "    \n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    " \n",
    "    conv1 = tf.nn.relu(bias, name=scope)\n",
    "     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:21:23.284775Z",
     "start_time": "2019-05-16T01:21:23.195692Z"
    }
   },
   "outputs": [],
   "source": [
    "def regression_model(inputs, is_training=True, scope=\"deep_regression\"):\n",
    "    \"\"\"Creates the regression model.\n",
    "\n",
    "    Args:\n",
    "        inputs: A node that yields a `Tensor` of size [batch_size, dimensions].\n",
    "        is_training: Whether or not we're currently training the model.\n",
    "        scope: An optional variable_op scope for the model.\n",
    "\n",
    "    Returns:\n",
    "        predictions: 1-D `Tensor` of shape [batch_size] of responses.\n",
    "        end_points: A dict of end points representing the hidden layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, 'deep_regression', [inputs]):\n",
    "        \n",
    "        end_points = {}\n",
    "        \n",
    "        # Set the default weight _regularizer and acvitation for each fully_connected layer.\n",
    "        # To set defaults parameters for a layer type\n",
    "        with slim.arg_scope([slim.fully_connected],\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            weights_regularizer=slim.l2_regularizer(0.01)):\n",
    "\n",
    "            # Creates a fully connected layer from the inputs with 32 hidden units.\n",
    "            net = slim.fully_connected(inputs, 32, scope='fc1')\n",
    "            end_points['fc1'] = net\n",
    "\n",
    "            # Adds a dropout layer to prevent over-fitting.\n",
    "            net = slim.dropout(net, 0.8, is_training=is_training)\n",
    "\n",
    "            # Adds another fully connected layer with 16 hidden units.\n",
    "            net = slim.fully_connected(net, 16, scope='fc2')\n",
    "            end_points['fc2'] = net\n",
    "\n",
    "            # Creates a fully-connected layer with a single hidden unit. Note that the\n",
    "            # layer is made linear by setting activation_fn=None.\n",
    "            predictions = slim.fully_connected(net, 1, activation_fn=None, scope='prediction')\n",
    "            end_points['out'] = predictions\n",
    "\n",
    "            return predictions, end_points\n",
    "\n",
    "        \n",
    "# Underatnd the model\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # Dummy placeholders for arbitrary number of 1d inputs and outputs\n",
    "    inputs = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    #outputs = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "    # Build model\n",
    "    predictions, end_points = regression_model(inputs)\n",
    "\n",
    "    # Print name and shape of each tensor.\n",
    "    print(\"Layers\")\n",
    "    for k, v in end_points.items():\n",
    "        print('name = {}, shape = {}'.format(v.name, v.get_shape()))\n",
    "\n",
    "    # Print name and shape of parameter nodes  (values not yet initialized)\n",
    "    print(\"\\n\")\n",
    "    print(\"Parameters\")\n",
    "    for v in slim.get_model_variables():\n",
    "        print('name = {}, shape = {}'.format(v.name, v.get_shape()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using slim.learning.train\n",
    "\n",
    "**Use slim training procedure is hard to get the learning curves and evaluations**. Its being criticized by the comunity. Might be why Google use slim to build the mode and for loop to train the model instead.\n",
    "\n",
    "```python\n",
    " final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=ckpt_dir,\n",
    "        number_of_steps=5000,\n",
    "        graph=graph,\n",
    "        save_summaries_secs=5,\n",
    "        log_every_n_steps=500)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:21:33.664955Z",
     "start_time": "2019-05-16T01:21:33.498995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create fake data to test the mode\n",
    "\n",
    "def produce_batch(batch_size, noise=0.3):\n",
    "    xs = np.random.random(size=[batch_size, 1]) * 10\n",
    "    ys = np.sin(xs) + 5 + np.random.normal(size=[batch_size, 1], scale=noise)\n",
    "    return [xs.astype(np.float32), ys.astype(np.float32)]\n",
    "\n",
    "# needs to run under with tf.Graph().as_default():\n",
    "def convert_data_to_tensors(x, y):\n",
    "    inputs = tf.constant(x)\n",
    "    inputs.set_shape([None, 1])\n",
    "    \n",
    "    outputs = tf.constant(y)\n",
    "    outputs.set_shape([None, 1])\n",
    "    return inputs, outputs\n",
    "\n",
    "x_train, y_train = produce_batch(200)\n",
    "x_test, y_test = produce_batch(200)\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can run and the checkpoints will be save and I can change the learn rate and run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:29:27.270861Z",
     "start_time": "2019-05-16T01:29:27.145069Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean up checkppoints\n",
    "!rm -vf tmp/regression_model/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:29:35.349598Z",
     "start_time": "2019-05-16T01:29:28.888727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a model\n",
    "ckpt_dir = 'tmp/regression_model/'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    inputs, targets = convert_data_to_tensors(x_train, y_train)\n",
    "\n",
    "    # Make the model.\n",
    "    predictions, nodes = regression_model(inputs, is_training=True)\n",
    "\n",
    "    # Add the loss function to the graph.\n",
    "    loss = tf.losses.mean_squared_error(labels=targets, predictions=predictions)\n",
    "    \n",
    "    # The total loss is the user's loss plus any regularization losses.\n",
    "    total_loss =  tf.losses.get_total_loss()# slim.losses.get_total_loss()\n",
    "\n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0075)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer) \n",
    "\n",
    "    # Run the training inside a session.\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=ckpt_dir,\n",
    "        number_of_steps=5000,\n",
    "        save_summaries_secs=5,\n",
    "        log_every_n_steps=500)\n",
    "\n",
    "print(\"Finished training. Last batch loss:\", final_loss)\n",
    "print(\"Checkpoint saved in %s\" % ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:42:56.919850Z",
     "start_time": "2019-05-16T01:42:50.392828Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    inputs, targets = convert_data_to_tensors(x_train, y_train)\n",
    "\n",
    "    # Make the model.\n",
    "    predictions, nodes = regression_model(inputs, is_training=True)\n",
    "\n",
    "    # Add the loss function to the graph.\n",
    "    loss = tf.losses.mean_squared_error(labels=targets, predictions=predictions)\n",
    "    \n",
    "    # The total loss is the user's loss plus any regularization losses.\n",
    "    total_loss =  tf.losses.get_total_loss()# slim.losses.get_total_loss()\n",
    "\n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer) \n",
    "\n",
    "    # Run the training inside a session.\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=ckpt_dir,\n",
    "        number_of_steps=5000 + 5000,\n",
    "        save_summaries_secs=5,\n",
    "        log_every_n_steps=500)\n",
    "\n",
    "print(\"Finished training. Last batch loss:\", final_loss)\n",
    "print(\"Checkpoint saved in %s\" % ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T02:52:04.044159Z",
     "start_time": "2019-05-15T02:52:03.912301Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls tmp/regression_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using slim to define layers and for loop for trainning\n",
    "\n",
    "Google authors of vggish modesl did that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:22:07.719614Z",
     "start_time": "2019-05-16T01:22:07.592625Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = produce_batch(200)\n",
    "x_test, y_test = produce_batch(200)\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:22:11.134807Z",
     "start_time": "2019-05-16T01:22:11.131444Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_examples_batch():\n",
    "    \n",
    "    chosen_id = np.random.randint(0,200, 200)\n",
    "  \n",
    "    return x_train[chosen_id], y_train[chosen_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T18:28:48.837690Z",
     "start_time": "2019-05-16T18:27:55.530254Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt_dir = 'tmp/regression_model2/'\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    with tf.variable_scope('mymodel'):\n",
    "    \n",
    "        #inputs, targets = convert_data_to_tensors(x_train, y_train)\n",
    "        inputs = tf.placeholder(tf.float32, shape=(None, 1), name='inputs')\n",
    "        targets = tf.placeholder(tf.float32, shape=(None, 1), name='targets')\n",
    "        \n",
    "        # Make the model.\n",
    "        predictions, nodes = regression_model(inputs, is_training=True)\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            \n",
    "            # You need this to save the model\n",
    "            global_step = tf.Variable(0, name='global_step', \n",
    "                                      trainable=False,\n",
    "                                    collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                     tf.GraphKeys.GLOBAL_STEP])\n",
    "            \n",
    "            # Add the loss function to the graph.\n",
    "            loss = tf.losses.mean_squared_error(labels=targets, predictions=predictions)\n",
    "    \n",
    "            # The total loss is the user's loss plus any regularization losses.\n",
    "            total_loss =  tf.losses.get_total_loss()# slim.losses.get_total_loss()\n",
    "            \n",
    "            # Specify the optimizer and create the train op:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)    \n",
    "            optimizer.minimize(total_loss,global_step=global_step, name='train_op')\n",
    "            \n",
    "            train_op = sess.graph.get_operation_by_name('mymodel/train/train_op')\n",
    "            \n",
    "            targets_tensor = sess.graph.get_tensor_by_name('mymodel/targets:0')\n",
    "            inputs_tensor = sess.graph.get_tensor_by_name('mymodel/inputs:0')\n",
    "            global_step_tensor = sess.graph.get_tensor_by_name('mymodel/train/global_step:0')\n",
    "\n",
    "    # Initialize all variables in the model, and then load the pre-trained\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    minibatch_size = 50\n",
    "    n_batches = 20000\n",
    "    costs= []\n",
    "    for _num_batches in range(n_batches):\n",
    "                    \n",
    "        # Dynamic creates systehetic data\n",
    "        (xt, yt) = _get_examples_batch()\n",
    "        \n",
    "        for i in range(0, 200, minibatch_size):\n",
    "        \n",
    "            # Get pair of (X, y) of the current minibatch/chunk\n",
    "            x_mini = xt[i:i + minibatch_size]\n",
    "            y_mini = yt[i:i + minibatch_size]\n",
    "\n",
    "            [loss, num_steps, _] = sess.run( [ total_loss, train_op, global_step_tensor],\n",
    "                                  feed_dict={inputs_tensor: x_mini, targets_tensor: y_mini })\n",
    "        \n",
    "        if _num_batches % 500 == 0:\n",
    "            costs.append(loss)\n",
    "            print(\"Epoch {}/{} loss: {}\".format(_num_batches + 1,n_batches, loss ))\n",
    "    \n",
    "    \n",
    "    # Prediction\n",
    "    #inputs, targets = convert_data_to_tensors(x_test, y_test)\n",
    "  \n",
    "    # Create the model structure. (Parameters will be loaded below.)\n",
    "    predictions, end_points = regression_model(inputs, is_training=False)\n",
    "\n",
    "#     # Make a session which restores the old parameters from a checkpoint.\n",
    "#     inputs, predictions, targets = sess.run([inputs, predictions, targets],\n",
    "#                                            feed_dict={inputs_tensor: x_test, targets_tensor: y_test })\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "plt.plot(costs, c='b')\n",
    "plt.plot(np.ones(len(costs))*0.12,c='r')\n",
    "\n",
    "# plt.scatter(inputs, targets, c='r');\n",
    "# plt.scatter(inputs, predictions, c='b');\n",
    "# plt.title('red=true, blue=predicted')\n",
    "    \n",
    "#     # Save model to disk.\n",
    "#     saver = tf.train.Saver()\n",
    "#     save_path = saver.save(sess,ckpt_dir, global_step = _num_batches)\n",
    "#     print(\"Model saved to {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T02:03:23.517717Z",
     "start_time": "2019-05-16T02:03:23.303829Z"
    }
   },
   "outputs": [],
   "source": [
    "# do a prediction\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    inputs, targets = convert_data_to_tensors(x_test, y_test)\n",
    "  \n",
    "    # Create the model structure. (Parameters will be loaded below.)\n",
    "    predictions, end_points = regression_model(inputs, is_training=False)\n",
    "\n",
    "    # Make a session which restores the old parameters from a checkpoint.\n",
    "    sv = tf.train.Supervisor(logdir=ckpt_dir)\n",
    "    with sv.managed_session() as sess:\n",
    "        inputs, predictions, targets = sess.run([inputs, predictions, targets])\n",
    "\n",
    "plt.scatter(inputs, targets, c='r');\n",
    "plt.scatter(inputs, predictions, c='b');\n",
    "plt.title('red=true, blue=predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Evaluation model\n",
    "\n",
    "TODO: NOT working Is hard or complicated to work with\n",
    "\n",
    "You have to run the script in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:09:35.721474Z",
     "start_time": "2019-05-15T03:09:35.495326Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_dir = 'tmp/log'\n",
    "\n",
    "# Choose the metrics to compute:\n",
    "names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    'RMSE Linear': slim.metrics.streaming_mean_squared_error(predictions=predictions, labels=targets),\n",
    "    'Mean Abs Error': slim.metrics.streaming_mean_absolute_error(predictions, targets)\n",
    "})\n",
    "\n",
    "\n",
    "# Create the summary ops such that they also print out to std output:\n",
    "summary_ops = []\n",
    "for metric_name, metric_value in names_to_values.items():\n",
    "     \n",
    "    op = tf.summary.scalar(metric_name, metric_value)\n",
    "    op = tf.Print(op, [metric_value], metric_name)\n",
    "    summary_ops.append(op)\n",
    "\n",
    "num_examples = 10000\n",
    "batch_size = 32\n",
    "num_batches = math.ceil(num_examples / float(batch_size))\n",
    "\n",
    "# Setup the global step.\n",
    "slim.get_or_create_global_step()\n",
    "\n",
    "output_dir = 'tmp/regression_model/' # Where the summaries are stored.\n",
    "eval_interval_secs = 0.5 # How often to run the evaluation.\n",
    "\n",
    "slim.evaluation.evaluation_loop(\n",
    "    'local',\n",
    "    ckpt_dir,\n",
    "    log_dir,\n",
    "    num_evals=num_batches,\n",
    "    eval_op=names_to_updates.values(),\n",
    "    summary_op=tf.summary.merge(summary_ops),\n",
    "    eval_interval_secs=eval_interval_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Convolutional (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:22:57.512166Z",
     "start_time": "2019-05-15T03:22:57.488726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datasets import flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T04:00:02.672036Z",
     "start_time": "2019-05-15T04:00:02.666604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_cnn(images, num_classes, is_training):  # is_training is not used...\n",
    "    \n",
    "    with slim.arg_scope([slim.max_pool2d], kernel_size=[3, 3], stride=2):\n",
    "        net = slim.conv2d(images, 64, [5, 5])\n",
    "        net = slim.max_pool2d(net)\n",
    "        net = slim.conv2d(net, 64, [5, 5])\n",
    "        net = slim.max_pool2d(net)\n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 192)\n",
    "        net = slim.fully_connected(net, num_classes, activation_fn=None)       \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T04:01:28.182830Z",
     "start_time": "2019-05-15T04:01:28.177477Z"
    },
    "hidden": true
   },
   "source": [
    "**Apply the model to some randomly generated images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T04:00:44.860880Z",
     "start_time": "2019-05-15T04:00:44.670398Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # The model can handle any input size because the first layer is convolutional.\n",
    "    # The size of the model is determined when image_node is first passed into the my_cnn function.\n",
    "    # Once the variables are initialized, the size of all the weight matrices is fixed.\n",
    "    # Because of the fully connected layers, this means that all subsequent images must have the same\n",
    "    # input size as the first image.\n",
    "    batch_size, height, width, channels = 3, 28, 28, 3\n",
    "    images = tf.random_uniform([batch_size, height, width, channels], maxval=1)\n",
    "    \n",
    "    # Create the model.\n",
    "    num_classes = 10\n",
    "    logits = my_cnn(images, num_classes, is_training=True)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Initialize all the variables (including parameters) randomly.\n",
    "    init_op = tf.global_variables_initializer()\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "        # Run the init_op, evaluate the model outputs and print the results:\n",
    "        sess.run(init_op)\n",
    "        probabilities = sess.run(probabilities)\n",
    "        \n",
    "print('Probabilities Shape:')\n",
    "print(probabilities.shape)  # batch_size x num_classes \n",
    "\n",
    "print('\\nProbabilities:')\n",
    "print(probabilities)\n",
    "\n",
    "print('\\nSumming across all classes (Should equal 1):')\n",
    "print(np.sum(probabilities, 1)) # Each row sums to 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialects",
   "language": "python",
   "name": "dialects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
