{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data\n",
    "\n",
    "STARTED\n",
    "\n",
    "1. Spark\n",
    "\n",
    "https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/course/\n",
    "\n",
    "2. tips is run OOM oout of memory (very good tips)\n",
    "\n",
    "https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/fe9a95cc542d4c30b855e632663c4797/8?activate_block_id=block-v1%3ABerkeleyX%2BCS105x%2B1T2016%2Btype%40vertical%2Bblock%4083ff2d3b4e93489b9b7b4861811e0872\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Sparks, pyspark n findspark\n",
    "\n",
    "1. needs docker\n",
    "2. needs edx course docker image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:52:32.181908Z",
     "start_time": "2019-04-22T19:52:32.177824Z"
    }
   },
   "source": [
    "```sh\n",
    "\n",
    "# Install images\n",
    "docker pull ucsddse230/cse255-dse230\n",
    "\n",
    "# Run container\n",
    "docker run --name ds_pragmatic -it -p 8890:8888 -v /media/leandroohf/sdb1/leandro/ds_pragmatic_programming:/home/ucsddse230/ ucsddse230/cse255-dse230 /bin/bash\n",
    "\n",
    "# Run jupyter inside container\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "1. http://localhost:8889/tree\n",
    "2. Copy and paste token to login in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/Weather/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T16:58:00.056165Z",
     "start_time": "2019-04-23T16:57:59.700606Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "%pylab inline\n",
    "\n",
    "#start the SparkContext\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark: Operations on (key,val) RDDs\n",
    "\n",
    "**Key/value pairs**\n",
    "\n",
    "* A python dictionary is a collection of *key/value* pairs.\n",
    "* The **key** is used to find a set of pairs with the particular key.\n",
    "* The **value** can be anything.\n",
    "* Spark has a set of special operations for *(key,value)* RDDs.\n",
    "\n",
    "* For more, see the spark [RDD programming guide]( http://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating from txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T20:07:23.963705Z",
     "start_time": "2019-04-22T20:07:23.778344Z"
    }
   },
   "outputs": [],
   "source": [
    "!head data/Moby-Dick.txt\n",
    "\n",
    "text_file = sc.textFile('data/Moby-Dick.txt')\n",
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Creating (key,value) RDDS\n",
    "\n",
    "**Method 1:** `parallelize` a list of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:49:20.704947Z",
     "start_time": "2019-04-22T18:49:20.185490Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pair_rdd = sc.parallelize([(1,2), (3,4)])\n",
    "print(pair_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Method 2:** `map` a function that maps elements to key/value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:49:30.195896Z",
     "start_time": "2019-04-22T18:49:29.459786Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "regular_rdd = sc.parallelize([1, 2, 3, 4, 2, 5, 6])\n",
    "pair_rdd = regular_rdd.map( lambda x: (x, x*x) )\n",
    "print(pair_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic operations\n",
    "\n",
    "**Properties of reduce operations**\n",
    "\n",
    "* Reduce operations **must not depend on the order**\n",
    "  * Order of operands should not matter\n",
    "  * Order of application of reduce operator should not matter\n",
    "\n",
    "* Multiplication and summation are good:\n",
    "\n",
    "```\n",
    "                1 + 3 + 5 + 2                      5 + 3 + 1 + 2 \n",
    "```\n",
    "\n",
    " * Division and subtraction are bad:\n",
    "\n",
    "```\n",
    "                    1 - 3 - 5 - 2                      1 - 3 - 5 - 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=sc.parallelize(range(3))\n",
    "\n",
    "L=A.collect()\n",
    "print(type(L))\n",
    "print(L)\n",
    "\n",
    "print()\n",
    "print(A.map(lambda x: x*x).collect())\n",
    "\n",
    "print()\n",
    "print(A.reduce(lambda x,y:x+y))\n",
    "\n",
    "# Does not work because depends of the order of execution wich you do not have control\n",
    "# Will return different when running in different times or machines\n",
    "print(A.reduce(lambda x,y:x-y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Transformations on (key,value) rdds. ** RDD $\\to$ RDD.\n",
    "  * **Examples** map, filter, sample, [More](http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)\n",
    "  * **No** communication needed.\n",
    "\n",
    "**Shuffles:** RDD $\\to$ RDD, **shuffle** needed\n",
    "\n",
    "**Shuffles are costly transfromations**\n",
    "\n",
    "* **Examples:** sort, distinct, repartition, sortByKey, reduceByKey, join [More](http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations)\n",
    "  * **A LOT** of communication might be needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### reduceByKey(func)\n",
    "Apply the reduce function on the values with the same key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:50:08.332003Z",
     "start_time": "2019-04-22T18:50:07.875187Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "print(\"After transformation : \", rdd.reduceByKey(lambda a,b: a+b).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that although it is similar to the reduce function, it is implemented as a transformation and not as an action because the dataset can have very large number of keys. So, it does not return values to the driver program. Instead, it returns a new RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### sortByKey()  and sortBy(): \n",
    "Sort RDD by keys in ascending order. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:51:15.023004Z",
     "start_time": "2019-04-22T18:51:14.616631Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(2,2), (1,4), (3,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "print(\"After transformation : \", rdd.sortByKey().collect())\n",
    "\n",
    "# Using sortBy\n",
    "print(\"After transformation : \", rdd.sortBy(lambda x: x[1],ascending=False).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Note:** The output of sortByKey() is an RDD. This means that  RDDs do have a meaningful order, which extends between partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### mapValues(func):\n",
    "Apply func to each value of RDD without changing the key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T18:51:52.580440Z",
     "start_time": "2019-04-22T18:51:52.442985Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "print(\"After transformation : \", rdd.mapValues(lambda x: x*2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### groupByKey(): \n",
    "Returns a new RDD of `(key,<iterator>)` pairs where the iterator iterates over the values associated with the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Iterators](http://anandology.com/python-practice-book/iterators.html) are python objects that generate a sequence of values. Writing a loop over `n` elements as \n",
    "```python\n",
    "for i in range(n):\n",
    "    ##do something\n",
    "```\n",
    "is inefficient because it first allocates a list of `n` elements and then iterates over it.\n",
    "Using the iterator `xrange(n)` achieves the same result without materializing the list. Instead, elements are generated on the fly.\n",
    "\n",
    "To materialize the list of values returned by an iterator we will use the list comprehension command:\n",
    "```python\n",
    "[a for a in <iterator>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "print(\"After transformation : \", rdd.groupByKey().mapValues(lambda x:[a for a in x]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original RDD :\", rdd.collect())\n",
    "\n",
    "# This sytanx is not working\n",
    "#print(rdd.groupByKey().map(lambda k, _iter: (k,[x for x in _iter ])).collect())\n",
    "print(rdd.groupByKey().map(lambda elem: (elem[0],[x for x in elem[1] ])).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:02:31.430080Z",
     "start_time": "2019-04-22T19:02:31.230897Z"
    }
   },
   "outputs": [],
   "source": [
    "# LHOF Notes\n",
    "rdd = sc.parallelize([(1,2), (2,4), (2,6), (2,7)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "# python 3 xrange = range and xrange is not defined anymore. This range is effcient\n",
    "print(\"len(iterator):  \", len(range(5)))\n",
    "print(\"After transformation : \", rdd.groupByKey().mapValues(lambda x: len(x)).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#####  flatMapValues(func): \n",
    "\n",
    "Similar to `flatMap()`: creates a separate key/value pair for each element of the list generated by the map operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`func` is a function that takes as input a single value and returns an iterator that generates a sequence of values.\n",
    "The application of flatMapValues operates on a key/value RDD. It applies `func` to each value, and gets an list (generated by the iterator) of values. It then combines each of the values with the original key to produce a list of key-value pairs. These lists are concatenated as in `flatMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:09:15.263353Z",
     "start_time": "2019-04-22T19:09:15.158484Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "\n",
    "# LHOF Notes\n",
    "x = 3\n",
    "print('list: ', list(range(x,x+2)))\n",
    "\n",
    "# the lambda function generates for each number i, an iterator that produces i,i+1\n",
    "print(\"After transformation : \", rdd.flatMapValues(lambda x: list(range(x,x+2))).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### (Advanced)  combineByKey(createCombiner, mergeValue, mergeCombiner): \n",
    "Combine values with the same key using a different result type.\n",
    "\n",
    "This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it. \n",
    "\n",
    "The elements of the original RDD are considered here *values*\n",
    "\n",
    "Values are converted into *combiners* which we will refer to here as \"accumulators\". An example of such a mapping is the mapping of the value *word* to the accumulator (*word*,1) that is done in WordCount.\n",
    "\n",
    "accumulators are then combined with values and the other combiner to generate a result for each key.\n",
    "\n",
    "For example, we can use it to calculate per-activity average durations as follows. Consider an RDD of key/value pairs where keys correspond to different activities and values correspond to duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:27:15.684133Z",
     "start_time": "2019-04-22T19:27:15.394030Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"Sleep\", 7), (\"Work\",5), (\"Play\", 3), \n",
    "                      (\"Sleep\", 6), (\"Work\",4), (\"Play\", 4),\n",
    "                      (\"Sleep\", 8), (\"Work\",5), (\"Play\", 5),(\"Sleep\", 2)])\n",
    "\n",
    "sum_counts = rdd.combineByKey(\n",
    "    (lambda x: (x, 1)), # createCombiner maps each value into a  combiner (or accumulator)\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)),\n",
    "#mergeValue defines how to merge a accumulator with a value (saves on mapping each value to an accumulator first)\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "\n",
    "print(rdd.collect())\n",
    "print(sum_counts.collect())\n",
    "duration_means_by_activity = sum_counts.mapValues(lambda value:\n",
    "                                                  value[0]*1.0/value[1]) \\\n",
    "                                            .collect()\n",
    "print(duration_means_by_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To understand combineByKey(), it’s useful to think of how it handles each element it processes. As combineByKey() traverses through the elements in a partition, each element either has a key it hasn’t seen before or has the same key as a previous element.\n",
    "\n",
    "If it’s a new key, createCombiner() is called to create the initial value for the accumulator on that key. In the above example, the accumulator is a tuple initialized as (x, 1) where x is a value in original RDD. Note that createCombiner() is called only when a key is seen for the first time in **each partition.**\n",
    "\n",
    "If it is a key we have seen before while processing that partition, it will instead use the provided function, mergeValue(), with the current value for the accumulator for that key and the new value.\n",
    "\n",
    "Since each partition is processed independently, we can have multiple accumulators for the same key. When we are merging the results from each partition, if two or more partitions have an accumulator for the same key, we merge the accumulators using the user-supplied mergeCombiners() function. In the above example, we are just adding the 2 accumulators element-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformations on two (key-value) RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:24:54.431365Z",
     "start_time": "2019-04-22T19:24:54.370126Z"
    },
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(1,2),(2,1),(2,2)])\n",
    "rdd2 = sc.parallelize([(2,5),(3,1)])\n",
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:22:41.309110Z",
     "start_time": "2019-04-22T19:22:40.598112Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# LHOF Notes\n",
    "\n",
    "rdd1 = sc.parallelize([(1,2), (2,1), (2,2)])\n",
    "rdd2 = sc.parallelize([(2,5), (3,1)])\n",
    "\n",
    "print('rdd1: ', rdd1.collect())\n",
    "print('rdd2: ', rdd2.collect())\n",
    "print('subtractByKey: ', rdd1.subtractByKey(rdd2).collect())\n",
    "\n",
    "print()\n",
    "# Pay attention. This is a set operation\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "\n",
    "print('x: ', x.collect())\n",
    "print('y: ', y.collect())\n",
    "print('subtract: ', sorted(x.subtract(y).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1. subtractByKey: \n",
    "Remove from RDD1 all elements whose key is present in RDD2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:24:58.332348Z",
     "start_time": "2019-04-22T19:24:57.986936Z"
    },
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())\n",
    "print(\"Result:\", rdd1.subtractByKey(rdd2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. join: \n",
    "* A fundamental operation in relational databases.\n",
    "* assumes two tables have a **key** column in common. \n",
    "* merges rows with the same key.\n",
    "\n",
    "Suppose we have two `(key,value)` datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|**dataset 1**|                                     |..........| **dataset 2** | \t       \t     |\n",
    "|-------------|-------------------------------------|   |-------------|-----------------|\n",
    "| **key=name**   |   **(gender,occupation,age)**    |   |  **key=name**   |   **hair color**    |\n",
    "| John   |  (male,cook,21)                          |   | Jill   |  blond |\n",
    "| Jill   |  (female,programmer,19)                  |   | Grace  |  brown |         \n",
    "| John   |  (male, kid, 2)                          |   | John   |  black |\n",
    "| Kate   |  (female, wrestler, 54)                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When `Join` is called on datasets of type `(Key, V)` and `(Key, W)`, it  returns a dataset of `(Key, (V, W))` pairs with all pairs of elements for each key. Joining the 2 datasets above yields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|   key = name | (gender,occupation,age),haircolor |\n",
    "|--------------|-----------------------------------|\n",
    "| John         | ((male,cook,21),black)             |\n",
    "| John         | ((male, kid, 2),black)             |\n",
    "| Jill         | ((female,programmer,19),blond)     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:28:29.605725Z",
     "start_time": "2019-04-22T19:28:29.229952Z"
    },
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())\n",
    "print(\"Result:\", rdd1.join(rdd2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "####  Variants of join.\n",
    "\n",
    "There are four variants of `join` which differ in how they treat keys that appear in one dataset but not the other.\n",
    "* `join` is an *inner* join which means that keys that appear only in one dataset are eliminated.\n",
    "* `leftOuterJoin` keeps all keys from the left dataset even if they don't appear in the right dataset. The result of leftOuterJoin in our example will contain the keys `John, Jill, Kate`\n",
    "* `rightOuterJoin` keeps all keys from the right dataset even if they don't appear in the left dataset. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John`\n",
    "* `FullOuterJoin` keeps all keys from both datasets. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John, Kate`\n",
    "\n",
    "In outer joins, if the element appears only in one dataset, the element in `(K,(V,W))` that does not appear in the dataset is represented bye `None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### 3. rightOuterJoin: \n",
    "Perform a right join between two RDDs. Every key in the right/second RDD will be present at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())\n",
    "print(\"Result:\", rdd1.rightOuterJoin(rdd2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### 4. leftOuterJoin: Perform a left join between two RDDs. Every key in the left RDD will be present at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())\n",
    "print(\"Result:\", rdd1.leftOuterJoin(rdd2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Actions on (key,val) RDDs. RDD $\\to$ Python-object in head node.\n",
    "\n",
    "  * **Examples:** reduce, collect, count, take, [More](http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n",
    "  * **Some** communication needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:31:27.390516Z",
     "start_time": "2019-04-22T19:31:27.382538Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1. countByKey(): \n",
    "Count the number of elements for each key. Returns a dictionary for easy access to keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T19:31:29.876146Z",
     "start_time": "2019-04-22T19:31:29.761264Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"RDD: \", rdd.collect())\n",
    "result = rdd.countByKey()\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2. collectAsMap(): \n",
    "Collect the result as a dictionary to provide easy lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# LHOF: Use only you make sure there is NO duplicated key. Because you will not know the end results otherwise \n",
    "# and you might get different results in different runs\n",
    "\n",
    "print(\"RDD: \", rdd.collect())\n",
    "result = rdd.collectAsMap()\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3. lookup(key): \n",
    "Return all values associated with the provided key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"RDD: \", rdd.collect())\n",
    "result = rdd.lookup(2 )\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counts (Hello word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurency of each word\n",
    "\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "not_empty = words.filter(lambda x: x!='') \n",
    "key_values= not_empty.map(lambda word: (word, 1)) \n",
    "counts=     key_values.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get thw top 5 most frequent word\n",
    "\n",
    "reverse_counts=counts.map(lambda x:(x[1],x[0]))   # reverse order of word and count\n",
    "sorted_counts=reverse_counts.sortByKey(ascending=False)\n",
    "\n",
    "print(sorted_counts.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes\n",
    "\n",
    "Dataframe are just RDDs with shema (metinfo). Because of the restrictions imposed by the schema, they can be more optmized\n",
    "\n",
    "\n",
    "sheet cheat:\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import Row\n",
    "# #from pyspark import sqlContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "rdd = sc.parallelize([Row(name=u\"John\",  age=19),\n",
    "                      Row(name=u\"Smith\", age=23),\n",
    "                      Row(name=u\"Sarah\", age=18)])\n",
    "\n",
    "rdd.collect()\n",
    "# Output:\n",
    "# [Row(name=u\"John\",  age=19),\n",
    "#  Row(name=u\"Smith\", age=23),\n",
    "#  Row(name=u\"Sarah\", age=18)]\n",
    "\n",
    "df = sqlContext.createDataFrame(rdd)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "rdd2 = sc.parallelize([(\"John\",19),(\"Smith\",23),(\"Sarah\",18)])\n",
    "\n",
    "\n",
    "schema = StructType([StructField(\"person_name\", StringType(),False),\n",
    "                     StructField(\"person_age\",  IntegerType(),False)\n",
    "])\n",
    "\n",
    "df2 = sqlContext.createDataFrame(rdd2,schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read and write from disk\n",
    "\n",
    "\n",
    "* parquet files (it is folder) are very popular e efficeent for IO in disk\n",
    "* parqet can be query directly from the disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parquet_file = 'data/users.parquet'\n",
    "print(parquet_file)\n",
    "\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parquet_file = 'data/users.parquet'\n",
    "print(parquet_file)\n",
    "\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!rm -rv data/df.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This throw an error if file already exit\n",
    "df.write.save(\"df.parquet\")\n",
    "!ls \n",
    "!echo\n",
    "!ls df.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Query parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT station,measurement,year \n",
      "FROM parquet.`data/Weather/NY.parquet` \n",
      "WHERE measurement=\"SNOW\" \n",
      "\n",
      "15629 ['station', 'measurement', 'year']\n",
      "+-----------+-----------+----+\n",
      "|    station|measurement|year|\n",
      "+-----------+-----------+----+\n",
      "|USC00308600|       SNOW|1932|\n",
      "|USC00308600|       SNOW|1956|\n",
      "|USC00308600|       SNOW|1957|\n",
      "|USC00308600|       SNOW|1958|\n",
      "|USC00308600|       SNOW|1959|\n",
      "+-----------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this way because the parquet file was not distributed in the hadoop cluster you do not have parallelism\n",
    "parquet_file = \"data/Weather/NY.parquet\"\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT station,measurement,year \n",
    "FROM parquet.`%s` \n",
    "WHERE measurement=\\\"SNOW\\\" \n",
    "\"\"\"%(parquet_file)\n",
    "\n",
    "print(query)\n",
    "\n",
    "df2 = sqlContext.sql(query)\n",
    "print(df2.count(),df2.columns)\n",
    "\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data wrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.select(\"name\",\"favorite_color\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!ls data/Weather\n",
    "\n",
    "# In case the files was not un compressed\n",
    "!tar -zxvf data/Weather/NY.tgz -C data/Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Weather/NY.parquet\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_file = 'data/Weather/NY.parquet'\n",
    "print(parquet_file)\n",
    "\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|    station|measurement|\n",
      "+-----------+-----------+\n",
      "|     168398|     168398|\n",
      "|       null|       null|\n",
      "|       null|       null|\n",
      "|USC00300015|       PRCP|\n",
      "|USW00094794|   TOBS_s20|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().select('station','measurement').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "* count(),  \n",
    "* avg(), max(), min()\n",
    "* approx_count_distinct() which uses sampling to get an approximate count fast. **<=That is very cool. Based on inference**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------+\n",
      "|measurement|min(year)|count(station)|\n",
      "+-----------+---------+--------------+\n",
      "|   TMIN_s20|     1873|         13442|\n",
      "|       TMIN|     1873|         13442|\n",
      "|   SNOW_s20|     1884|         15629|\n",
      "|       TOBS|     1876|         10956|\n",
      "|   SNWD_s20|     1888|         14617|\n",
      "+-----------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('measurement').agg({'year': 'min', 'station':'count'}).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 343\n",
      "+------------------------------+\n",
      "|approx_count_distinct(station)|\n",
      "+------------------------------+\n",
      "|                           339|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"True value: {}\".format(df.select('station').distinct().count()))\n",
    "df.agg({'station':'approx_count_distinct'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T16:57:46.988545Z",
     "start_time": "2019-04-23T16:57:46.906960Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/people.json\"\n",
    "\n",
    "# Create a DataFrame from the file(s) pointed to by path\n",
    "people = sqlContext.read.json(path)\n",
    "#print('people is a',type(people))\n",
    "\n",
    "print(people.count())\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method.\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Needs to register the dataframe first\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "for each in teenagers.collect():\n",
    "    print(each[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataframeStatFunctions\n",
    "\n",
    "Methods for statistics functionality. [documented here](http://takwatanabe.me/pyspark/generated/generated/pyspark.sql.DataFrameStatFunctions.html)\n",
    "\n",
    "**Some of these functionso use inference to get aproximated estimations**\n",
    "\n",
    "* **approxQuantile(col, probabilities, relativeError)**\tCalculates the approximate quantiles of a numerical column of a DataFrame.\n",
    "* **corr(col1, col2[, method])**\tCalculates the correlation of two columns of a DataFrame as a double value.\n",
    "* **cov(col1, col2)**\tCalculate the sample covariance for the given columns, specified by their names, as a double value.\n",
    "* **crosstab(col1, col2)**\tComputes a pair-wise frequency table of the given columns.\n",
    "* **freqItems(cols[, support])**\tFinding frequent items for columns, possibly with false positives.\n",
    "* **sampleBy(col, fractions[, seed])**\tReturns a stratified sample without replacement based on the fraction given on each stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9]\n",
      "\n",
      "Get the qiantiles\n",
      "with accuracy 0.1:  [1871.0, 1926.0, 1947.0, 1957.0, 1966.0, 1980.0, 1985.0, 1993.0, 2013.0]\n",
      "with accuracy 0.01:  [1917.0, 1938.0, 1949.0, 1957.0, 1966.0, 1975.0, 1984.0, 1993.0, 2003.0]\n"
     ]
    }
   ],
   "source": [
    "# This make the quantiles\n",
    "print([0.1*i for i in range(1,10)])\n",
    "\n",
    "\n",
    "# That is very cool. Based on inference**\n",
    "print()\n",
    "print(\"Get the qiantiles\")\n",
    "print('with accuracy 0.1: ',df.approxQuantile('year', [0.1*i for i in range(1,10)], 0.1))\n",
    "print('with accuracy 0.01: ',df.approxQuantile('year', [0.1*i for i in range(1,10)], 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes UDF\n",
    "\n",
    "    User define functions *UDF* are functions that can be ran against a\n",
    "    dataframe and spark can optimize it\n",
    "\n",
    "    Steps:\n",
    "    1. define function that operates in the value field\n",
    "    2. register the function\n",
    "    3. use the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def packArray(a):\n",
    "    \"\"\"\n",
    "    pack a numpy array into a bytearray that can be stored as a single \n",
    "    field in a spark DataFrame\n",
    "\n",
    "    :param a: a numpy ndarray \n",
    "    :returns: a bytearray\n",
    "    :rtype:\n",
    "\n",
    "    \"\"\"\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "\n",
    "def unpackArray(x,data_type=np.float16):\n",
    "    \"\"\"\n",
    "    unpack a bytearray into a numpy.ndarray\n",
    "\n",
    "    :param x: a bytearray\n",
    "    :param data_type: The dtype of the array. This is important because if determines how many bytes go into each entry in the array.\n",
    "    :returns: a numpy array\n",
    "    :rtype: a numpy ndarray of dtype data_type.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.frombuffer(x,dtype=data_type)\n",
    "\n",
    "def Count_nan(V):\n",
    "\n",
    "    A=unpackArray(V,data_type=np.float16)\n",
    "\n",
    "    return int(sum(np.isnan(A)))\n",
    "\n",
    "#register\n",
    "Count_nan_udf = udf(Count_nan, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"na_no\", Count_nan_udf(df.Values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----------+-----+\n",
      "|    Station|Year|Measurement|na_no|\n",
      "+-----------+----+-----------+-----+\n",
      "|USW00094704|1945|   PRCP_s20|  204|\n",
      "|USW00094704|1946|   PRCP_s20|    0|\n",
      "|USW00094704|1947|   PRCP_s20|  143|\n",
      "|USW00094704|1948|   PRCP_s20|    0|\n",
      "|USW00094704|1949|   PRCP_s20|    0|\n",
      "+-----------+----+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Station\",\"Year\",\"Measurement\",\"na_no\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
