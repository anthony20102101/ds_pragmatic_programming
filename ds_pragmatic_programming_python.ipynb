{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T18:09:18.303884Z",
     "start_time": "2018-09-11T18:09:18.300979Z"
    }
   },
   "source": [
    "# Data Science Pragmatic programming in Python\n",
    "\n",
    "\n",
    "Code snippets for quick consult (copy n paste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:42:52.066285Z",
     "start_time": "2019-10-21T00:42:49.806327Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T23:05:49.950384Z",
     "start_time": "2019-09-26T23:05:49.831303Z"
    }
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T21:25:13.342795Z",
     "start_time": "2019-04-01T21:25:13.320892Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "# take random sample of lines in file (log analysys was useful)\n",
    "\n",
    "shuf data/FremontBridge.csv | head -n 3 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Filter with grep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T21:27:20.017282Z",
     "start_time": "2019-04-01T21:27:20.000616Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Use grep to filter out pattern\n",
    "cat data/FremontBridge.csv | grep -v 2013 | head -n 5\n",
    "\n",
    "echo\n",
    "# keep only 2013 and ignore case sensitive\n",
    "cat data/FremontBridge.csv | grep -i 2013 | head -n 5\n",
    "\n",
    "# Using regular expression (start with 12)\n",
    "echo\n",
    "cat data/FremontBridge.csv | grep -E \"^12\" | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### run process in parralell\n",
    "\n",
    "Use pararlels to speedup bash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:27:20.071236Z",
     "start_time": "2019-05-15T19:27:19.743042Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating scripts\n",
    "\n",
    "bash_str = \"\"\"\\\n",
    "ffmpeg -y -i data/audio0_hH79HnEdo.wav data/audio0_hH79HnEdo.flac\n",
    "ffmpeg -y -i data/audio0KCVgexi4yU.wav data/audio0KCVgexi4yU.flac\n",
    "ffmpeg -y -i data/audio0Q1JLNfm8oU.wav data/audio0Q1JLNfm8oU.flac\n",
    "ffmpeg -y -i data/audio0vm7UpkSOnk.wav data/audio0vm7UpkSOnk.flac\n",
    "ffmpeg -y -i data/audio2M9GSksX_ho.wav data/audio2M9GSksX_ho.flac\n",
    "\"\"\"\n",
    "\n",
    "!echo \"{bash_str}\"   > convert.sh \n",
    "!chmod u+x convert.sh\n",
    "\n",
    "!cat convert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:29:38.571698Z",
     "start_time": "2019-05-15T19:29:38.031925Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# serial\n",
    "!time ./convert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:29:44.882312Z",
     "start_time": "2019-05-15T19:29:44.526899Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parallel will run every line as a separate command in parallel\n",
    "!/usr/bin//parallel -a ./convert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:31:53.445648Z",
     "start_time": "2019-05-15T19:31:53.118739Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# piping version\n",
    "!ls data/*.wav | parallel ffmpeg -i {} {.}.flac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T20:56:53.499721Z",
     "start_time": "2019-03-07T20:56:53.490921Z"
    }
   },
   "source": [
    "## Notebooks tricks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Install conda enviroment in jupyter notebook *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set jupyter notebook to save a copy as Rmd\n",
    "\n",
    "refs: https://towardsdatascience.com/version-control-with-jupyter-notebooks-f096f4d7035a\n",
    "\n",
    "Why?  \n",
    "\n",
    "* R-Markdown is the same as markdown format with added advantage that you can make publishing worthy pdf/word-doc \n",
    "* git diff renders well is human readable (Cons duplicated the numner of files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Install pytext (requirment to save as Rmd)\n",
    "```sh\n",
    "pip install jupytext --upgrade\n",
    "```\n",
    "\n",
    "Edit jupyter conf file: jupyter/jupyter_notebook_config.py\n",
    "\n",
    "```txt\n",
    "c.NotebookApp.contents_manager_class=\"jupytext.TextFileContentsManager\"\n",
    "c.ContentsManager.default_jupytext_formats = \".ipynb,.Rmd\"\n",
    "```\n",
    "\n",
    "\n",
    "Add this magic command in the first cell\n",
    "\n",
    "\n",
    "```python\n",
    "%autosave 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Install kernel enviroment in jupyter notebook *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Run notebook in remote jupyter server*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-05T18:58:25.531398Z",
     "start_time": "2019-04-05T18:58:25.523618Z"
    }
   },
   "source": [
    "\n",
    "```sh\n",
    "# run in thew remote machine\n",
    "jupyter notebook --no-browser --port 8889\n",
    "\n",
    "# run local machine\n",
    "ssh -N -f -L localhost:8888:localhost:8889 datalab@172.21.1.4\n",
    "\n",
    "# Copy token from the remote machine shell\n",
    "# access jupyter notebook\n",
    "http://localhost:8888/?token=054a85cc962d8c7ee5ba99517bfc5303020c7e2736f4b268\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T20:57:43.950467Z",
     "start_time": "2019-03-07T20:57:43.942384Z"
    }
   },
   "source": [
    "This can help with interacting with shell\n",
    "* share variables between python and bash cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T23:05:53.495314Z",
     "start_time": "2019-09-26T23:05:53.382429Z"
    }
   },
   "outputs": [],
   "source": [
    "python_var = 'test1'\n",
    "var_test2 = 10.3\n",
    "\n",
    "# Option 1\n",
    "data =  './mydata'\n",
    "\n",
    "!mkdir -p {data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T23:05:54.197567Z",
     "start_time": "2019-09-26T23:05:54.183281Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$python_var\" \"$var_test2\" \"position_3\"\n",
    "\n",
    "echo $1\n",
    "echo $2\n",
    "echo $3\n",
    "echo $data ## will not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define enviroment variable shared between bash cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T21:00:39.525129Z",
     "start_time": "2019-03-07T21:00:39.520079Z"
    }
   },
   "outputs": [],
   "source": [
    "%env MY_VAR=env_test_var\n",
    "%env  v1=$python_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T21:00:40.608121Z",
     "start_time": "2019-03-07T21:00:40.600269Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "echo $MY_VAR\n",
    "echo $v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Share variables between notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:32:18.300352Z",
     "start_time": "2019-10-21T00:32:18.293721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook 1 (Should be ran first)\n",
    "# Define the var\n",
    "share = 'This is shared variable'\n",
    "\n",
    "\n",
    "# Make the varaible Global between notebooks\n",
    "%store share \n",
    "\n",
    "# Notebook 2 \n",
    "# Read global variable\n",
    "%store -r share\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Run R code in cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T21:14:23.345159Z",
     "start_time": "2019-05-07T21:14:22.886530Z"
    }
   },
   "outputs": [],
   "source": [
    "# enables the %%R magic, not necessary if you've already done this\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'cups_of_coffee': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'productivity': [2, 5, 6, 8, 9, 8, 0, 1, 0, -1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T21:14:25.873300Z",
     "start_time": "2019-05-07T21:14:23.541877Z"
    }
   },
   "outputs": [],
   "source": [
    "%%R -i df -w 3 -h 2 --units in -r 200\n",
    "# import df from global environment\n",
    "# make default figure size 3 by 3 inches with 200 dpi resolution\n",
    "\n",
    "install.packages(\"ggplot2\", repos='http://cran.us.r-project.org', quiet=TRUE)\n",
    "library(ggplot2)\n",
    "ggplot(df, aes(x=cups_of_coffee, y=productivity)) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate data \n",
    "\n",
    "* random numbers generation\n",
    "* generate data\n",
    "* load data sets (toy)\n",
    "\n",
    "http://scikit-learn.org/stable/datasets/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gen random numbers\n",
    "\n",
    "* uniform\n",
    "* normal\n",
    "* multivariate\n",
    "* bolean\n",
    "* integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T00:53:28.393935Z",
     "start_time": "2018-09-12T00:53:28.346725Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## random numbers\n",
    "np.random.seed(seed=2018)\n",
    "\n",
    "n = 3\n",
    "a = np.random.rand(n,n)\n",
    "\n",
    "# random boolean\n",
    "b = np.random.uniform(size=3) > .5\n",
    "b\n",
    "\n",
    "# uniform\n",
    "u = np.random.uniform(size=3)\n",
    "u\n",
    "\n",
    "# random int\n",
    "i = np.random.randint(0,9,size=3)\n",
    "i\n",
    "\n",
    "# random choice\n",
    "c = np.random.choice(['a','b','c','e'],size=7)\n",
    "c\n",
    "\n",
    "# normal\n",
    "n = np.random.normal(size=5)\n",
    "n\n",
    "\n",
    "# multivariated normal 2d;\n",
    "_mean = (1, 2)\n",
    "_cov = [[1, 0], [0, 1]]\n",
    "m = np.random.multivariate_normal(_mean, _cov, 3 )\n",
    "m.shape\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T03:55:37.789517Z",
     "start_time": "2019-03-27T03:55:37.757067Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n",
    "\n",
    "pd.DataFrame.from_dict(data)\n",
    "\n",
    "\n",
    "data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n",
    "pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "\n",
    "\n",
    "# HOW to fixe: If using all scalar values, you must pass an index\n",
    "# pd.DataFrame.from_dict({'col1': 1, 'col2': 3}) # <= RUN THIS LINE TO SEE THE ERROR\n",
    "#pd.DataFrame({'A': 1, 'B': 2}) # # <= RUN THIS LINE TO SEE THE ERROR\n",
    "pd.DataFrame({'A': 1, 'B': 2}, index=[0])\n",
    "\n",
    "# OR\n",
    "pd.DataFrame({'A': [1], 'B': [2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gen data for classifications and clustering\n",
    "\n",
    "Shameless stolen from:\n",
    "http://scikit-learn.org/stable/auto_examples/datasets/plot_random_dataset.html#sphx-glr-auto-examples-datasets-plot-random-dataset-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **make_classification** create multiclass datasets by allocating each class one or more **normally-distributed** clusters of points\n",
    "\n",
    "   make_classification introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T07:41:12.482847Z",
     "start_time": "2018-12-03T07:41:11.864863Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## classification (normal distributed)\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "plt.figure(figsize=(10, 15));\n",
    "\n",
    "plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95);\n",
    "\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,\n",
    "                             n_clusters_per_class=1)\n",
    "\n",
    "plt.subplot(321);\n",
    "plt.title(\"One informative feature, one cluster per class\", fontsize='small');\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1)\n",
    "\n",
    "plt.subplot(322);\n",
    "plt.title(\"Two informative features, one cluster per class\", fontsize='small');\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k');\n",
    "\n",
    "\n",
    "X2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)\n",
    "\n",
    "plt.subplot(323);\n",
    "plt.title(\"Two informative features, two clusters per class\",\n",
    "          fontsize='small');\n",
    "\n",
    "plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1, n_classes=3)\n",
    "\n",
    "plt.subplot(324);\n",
    "plt.title(\"Multi-class, two informative features, one cluster\",\n",
    "          fontsize='small');\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k');\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T18:37:11.959944Z",
     "start_time": "2018-09-11T18:37:11.956913Z"
    },
    "hidden": true
   },
   "source": [
    "* **make_blobs**: Generate isotropic (uniformity in all orientations) Gaussian blobs for clustering. \n",
    "\n",
    "* **make_gaussian_quantiles**: Generate isotropic Gaussian and label samples by quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T00:53:37.806512Z",
     "start_time": "2018-09-12T00:53:37.536558Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Blobs n gaussian qunatiles\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "X1, Y1 = make_blobs(n_features=2, centers=3, random_state=2018)\n",
    "\n",
    "plt.figure(figsize=(11,5));\n",
    "\n",
    "ax1 = plt.subplot(121);\n",
    "plt.title(\"Three blobs\", fontsize='small');\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k');\n",
    "\n",
    "X1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3,n_samples=256,random_state=2018)\n",
    "\n",
    "ax2 = plt.subplot(122);\n",
    "plt.title(\"Gaussian divided into three quantiles\", fontsize='small');\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k');\n",
    "\n",
    "ax2.set_xlim(-4,4);\n",
    "ax2.set_ylim(-4,4);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **make_circles**: Make a large circle containing a smaller circle in 2d.\n",
    "\n",
    "* **make_moons**: Make two interleaving half circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T00:53:41.025344Z",
     "start_time": "2018-09-12T00:53:40.817521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Clustering\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05, random_state=2018)\n",
    "\n",
    "# scatter plot of original and reduced data\n",
    "fig = plt.figure(figsize=(9, 8));\n",
    "\n",
    "ax1 = plt.subplot(121,aspect='equal');\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k');\n",
    "ax1.set_title(\"Circle Data (2d)\");\n",
    "ax1.set_xticks(());\n",
    "ax1.set_yticks(());\n",
    "\n",
    "\n",
    "X, y = make_moons(n_samples=400, noise=.05,random_state=2018)\n",
    "\n",
    "ax2 = plt.subplot(122,aspect='equal');\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k');\n",
    "ax2.set_title(\"Moons Data (2d)\");\n",
    "ax2.set_xticks(());\n",
    "ax2.set_yticks(());\n",
    "\n",
    "plt.tight_layout();\n",
    "plt.show;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load data sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **load_iris** Load and return the iris dataset (classification).\n",
    "\n",
    "<img src=\"images/iris_petal_sepal.png\" width=\"250\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T00:53:47.632419Z",
     "start_time": "2018-09-12T00:53:47.487355Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## datasets\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris flower dataset:\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "Y_iris = iris.target\n",
    "\n",
    "print('data size: {}'.format(X_iris.shape))\n",
    "print('features: {}'.format(iris.feature_names))\n",
    "print('labels: {}'.format(iris.target_names))\n",
    "\n",
    "print()\n",
    "\n",
    "plt.figure(2, figsize=(4, 3));\n",
    "plt.clf();\n",
    "\n",
    "# Plot \n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 1], c=Y_iris, cmap=plt.cm.Set1,\n",
    "            edgecolor='k');\n",
    "plt.xlabel('Sepal length');\n",
    "plt.ylabel('Sepal width');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* **load_boston**: Load and return the boston house-prices dataset (regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T18:34:37.108941Z",
     "start_time": "2018-10-26T18:34:37.071503Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Boston house pricing\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "print('data size: {}'.format(boston.data.shape))\n",
    "print('features: {}'.format(boston.feature_names))\n",
    "\n",
    "# convert to data frame (but may times is not necessary)\n",
    "boston_df = pd.DataFrame(boston.data)\n",
    "boston_df.columns = boston.feature_names\n",
    "\n",
    "boston_df['target'] = boston.target\n",
    "\n",
    "boston_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T18:26:47.338288Z",
     "start_time": "2018-10-26T18:26:47.041497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/phone_data.csv')\n",
    "\n",
    "data.shape\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and write data\n",
    "\n",
    "* pandas\n",
    "* dict\n",
    "* file handle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T23:24:46.074944Z",
     "start_time": "2018-11-15T23:24:46.071949Z"
    }
   },
   "source": [
    "### Pandas\n",
    "\n",
    "* **read from compressed csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:26:50.585962Z",
     "start_time": "2019-06-25T21:26:50.158036Z"
    }
   },
   "outputs": [],
   "source": [
    "# See the first rows\n",
    "pd.read_csv('data/phone_data.csv', nrows=2).head()\n",
    "pd.read_csv('data/phone_data.csv', sep=',',header=None, index_col=False).head()\n",
    "\n",
    "## load\n",
    "creditcard_df = pd.read_csv('data/creditcard_downsampled.csv.tar.bz2', compression='bz2', \n",
    "                            sep=',', quotechar='\"', index_col=False)\n",
    "\n",
    "creditcard_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **read from sql db**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T23:23:36.850539Z",
     "start_time": "2018-11-15T23:23:36.702654Z"
    }
   },
   "outputs": [],
   "source": [
    "# reads from database\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "#conn = sqlite3.connect(\"data/flights.db\")\n",
    "with sqlite3.connect(\"data/flights.db\") as conn:\n",
    "    df = pd.read_sql_query(\"select * from airlines limit 11;\", conn)\n",
    "    \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T22:26:26.639702Z",
     "start_time": "2019-05-09T22:26:26.613274Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/phone_data.csv', index_col=False)\n",
    "\n",
    "df.columns\n",
    "df.drop('index',axis=1,inplace=True)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T22:26:53.499544Z",
     "start_time": "2019-05-09T22:26:53.375557Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('tmp.csv',index=False)\n",
    "\n",
    "!head tmp.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic and dict IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* **process all files in folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ds_pragmatic_programming_modelling.ipynb',\n",
       " 'ds_pragmatic_programming_NLP.ipynb',\n",
       " 'ds_pragmatic_programming_code_best_practices.ipynb',\n",
       " 'ds_pragmatic_programming_python.ipynb',\n",
       " 'ds_pragmatic_programming_python_time_series.ipynb',\n",
       " 'ds_pragmatic_programming_python_visualization.ipynb',\n",
       " 'ds_pragamatic_mongo_db.ipynb',\n",
       " 'ds_pragmatic_programming_SQL.ipynb',\n",
       " 'ds_pragmatic_programming_deeplearning.ipynb',\n",
       " 'ds_pragmatic_programming_geodata.ipynb',\n",
       " 'ds_pragmatic_programming_graph_data.ipynb',\n",
       " 'ds_pragmatic_programming_spark.ipynb']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling.png\n",
      "minibatch_learning_rate.png\n",
      "pivot-table-datasheet.png\n",
      "neuron_ANN.png\n",
      "data_frame.png\n",
      "iris_petal_sepal.png\n",
      "tomek.png\n",
      "non-linear_and_linear_decision_edge.png\n",
      "learning_rate2.png\n",
      "smote.png\n",
      "no_data_pipeline.png\n",
      "pathlib_cheatsheet_p1.png\n",
      "with_data_pipeline.png\n",
      "refactor_notebooks.png\n",
      "biasvariance.png\n",
      "hig_bias_low_variance.png\n",
      "learning_rate.png\n",
      "loss_learning_rate.png\n",
      "split-apply-combine.png\n",
      "low_high_var.png\n",
      "irr_error.png\n",
      "onehot.png\n",
      "notebook_vs_code.png\n",
      "add_data.png\n",
      "\n",
      "tensorflow_requirements.txt\n",
      "ds_pragmatic_programming_modelling.ipynb\n",
      "data\n",
      "ds_pragmatic_programming_NLP.ipynb\n",
      ".git\n",
      "ds_pragmatic_programming_code_best_practices.ipynb\n",
      ".ipynb_checkpoints\n",
      "ds_pragmatic_programming_python.ipynb\n",
      "ds_pragmatic_programming_python_time_series.ipynb\n",
      "requirements.txt\n",
      "postBuild\n",
      "ds_pragmatic_programming_python_visualization.ipynb\n",
      "ds_pragamatic_mongo_db.ipynb\n",
      "ds_pragmatic_programming_SQL.ipynb\n",
      "tensorflow_cpu.yml\n",
      "ds_pragmatic_programming_deeplearning.ipynb\n",
      "README.md\n",
      "Dockerfile-spark\n",
      "ds_pragmatic_programming_geodata.ipynb\n",
      "Dockerfile\n",
      "ds_pragmatic_programming_graph_data.ipynb\n",
      "ds_pragmatic_programming_spark.ipynb\n",
      "images\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "notebooks = [x for x in glob.glob('*.ipynb')]\n",
    "\n",
    "notebooks\n",
    "\n",
    "for name in glob.glob('images/*.png'):\n",
    "    print(os.path.basename(name))\n",
    "\n",
    "    \n",
    "print()\n",
    "\n",
    "directory = './'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **dump dict to json file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T19:05:25.027121Z",
     "start_time": "2019-05-08T19:05:24.900128Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {}  \n",
    "data['people'] = []  \n",
    "data['people'].append({  \n",
    "    'name': 'Scott',\n",
    "    'website': 'stackabuse.com',\n",
    "    'from': 'Nebraska'\n",
    "})\n",
    "\n",
    "with open('data.json', 'w') as outfile:  \n",
    "    json.dump(data, outfile)\n",
    "    \n",
    "!cat data.json\n",
    "\n",
    "with open('data.json', 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "    \n",
    "datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -v data/output.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Write txt file line by line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazzy way\n",
    "for k in range(0,3):\n",
    "    \n",
    "    if k % 2 == 0:\n",
    "        print('Processing: {}'.format(k))\n",
    "\n",
    "    score =  np.random.normal(size=1)[0]\n",
    "    print(\"{k:d}\\t{score:.2f}\".format(k=k, score=score), file=open(\"data/output.tsv\", \"a\"))\n",
    "\n",
    "# better way\n",
    "with open('data/output.txt', 'a') as f:\n",
    "    for k in range(0,3):\n",
    "    \n",
    "        if k % 2 == 0:\n",
    "            print('Processing: {}'.format(k))\n",
    "\n",
    "        score =  np.random.normal(size=1)[0]\n",
    "        print(\"{k:d}\\t{score:.2f}\".format(k=k, score=score), file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/output.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "my_dict = {'key1': 'value_a', 'key2': 1, 'key3': 2.0}\n",
    "\n",
    "# write\n",
    "with open('data/dict.csv', 'w') as f:\n",
    "    [f.write('{0},{1}\\n'.format(key, value)) for key, value in my_dict.items()];\n",
    "\n",
    "\n",
    "# read\n",
    "with open('data/dict.csv', 'r') as csv_file:\n",
    "    \n",
    "    reader = csv.reader(csv_file)\n",
    "    my_dict2 = dict(reader)\n",
    "    \n",
    "my_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/dict.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = [ {'key1': 1, 'key2': 'a'}, {'key1': 2, 'key2': 'b'}]\n",
    "\n",
    "fieldnames = dict_list[0].keys()\n",
    "\n",
    "# write\n",
    "with open('data/people.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(dict_list)\n",
    "    \n",
    "# read\n",
    "with open('data/people.csv', mode='r') as infile:\n",
    "    \n",
    "    reader = csv.reader(infile)\n",
    "    _ = next(reader, None)  # skip the headers\n",
    "    mydict = [{'key1': rows[0], 'key2': rows[1]} for rows in reader]\n",
    "    \n",
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/people.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from internet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T05:01:30.751603Z",
     "start_time": "2019-03-08T05:01:27.275688Z"
    }
   },
   "outputs": [],
   "source": [
    "!curl -o data/FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:43:13.233739Z",
     "start_time": "2019-10-21T00:43:02.056521Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True)\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangler (Remember, 80% of the project time is spent here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulations\n",
    "\n",
    "* cut: segment and sort data values into bins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "pd.cut(np.array([1, 7, 5, 4, 6, 3]),3)\n",
    "\n",
    "# with want list instead of tuples\n",
    "pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True)\n",
    "\n",
    "# returns labels as well\n",
    "pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, labels=[\"bad\", \"medium\", \"good\"]) \n",
    "\n",
    "# if you only wants the bins numbers\n",
    "pd.cut([0, 1, 1, 2], bins=4, labels=False)\n",
    "\n",
    "# passing pd.Series return pd.Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### operating with list (very sueful)\n",
    "\n",
    "http://book.pythontips.com/en/latest/map_filter.html\n",
    "\n",
    "* map\n",
    "* filter\n",
    "* reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:43:22.488964Z",
     "start_time": "2019-10-21T00:43:22.472761Z"
    }
   },
   "outputs": [],
   "source": [
    "items = [1, 2, 3, 4, 5]\n",
    "squared = list(map(lambda x: x**2, items))\n",
    "squared\n",
    "\n",
    "all_rows = [(1,1,'segid1', 3.0),(1,1,'segid2', 1.0),(2,2,'segid3', 4.0)]\n",
    "list(filter(lambda t: t[1] == 1, all_rows))\n",
    "\n",
    "from functools import reduce\n",
    "product = reduce((lambda x, y: x * y), [1, 2, 3, 4])\n",
    "product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing or selection\n",
    "\n",
    "pandas terminology are the indexers that do selections or subsetting: \"[]\", .loc and .iloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refs:\n",
    "\n",
    "[pandas cheat sheet](http://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "\n",
    "\n",
    "https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c  \n",
    "\n",
    "https://github.com/tdpetrou/Learn-Pandas\n",
    "\n",
    "<img src=\"images/data_frame.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:43:45.671413Z",
     "start_time": "2019-10-21T00:43:45.642397Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "'state': ['NY','TX','FL','AL','AK','TX','TX' ],\n",
    "'color': ['blue','green','red','white','gray','black','red'],\n",
    "'food': [ 'Steak', 'Lamb', 'Mango', 'Apple', 'Cheese', 'Melon', 'Beans' ],\n",
    "'age': [ 30, 2, 12, 4, 32, 33, 69 ],\n",
    "'height': [ 165, 70, 120, 80, 180, 172, 150 ],\n",
    "'score': [ 4.6, 8.3, 9.0, 3.3, 1.8, 9.5, 2.2 ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, )\n",
    "df.index = ['Jane', 'Niko', 'Aaron', 'Penelope', 'Dean', 'Christina', 'Cornelia' ]\n",
    "\n",
    "values = df.values\n",
    "df\n",
    "\n",
    "df.columns\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *indexing operator: []**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:43:49.949151Z",
     "start_time": "2019-10-21T00:43:49.914888Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns a series\n",
    "# A Series is a one-dimensional sequence of labeled data. \n",
    "df['food']\n",
    "    \n",
    "# Select multiple columns as a DataFrame by passing a list to it\n",
    "# return dataframe\n",
    "# the order of the column does not matter for selecting\n",
    "df[['color', 'food', 'score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.loc** subset rows and columns\n",
    "\n",
    "only selects data by the LABEL of the rows and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:43:53.890488Z",
     "start_time": "2019-10-21T00:43:53.795834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select rows  n returns serries\n",
    "df.loc['Niko']\n",
    "\n",
    "# select mutliple rows n return data frames\n",
    "df.loc[['Niko', 'Penelope']]\n",
    "\n",
    "\n",
    "# Slicing between range of rows\n",
    "df.loc['Niko':'Dean']\n",
    "\n",
    "# from begin to Aaron\n",
    "df.loc[:'Aaron']\n",
    "\n",
    "# from Dean to end\n",
    "df.loc['Dean':]\n",
    "\n",
    "# controling the step\n",
    "df.loc['Niko':'Christina':2]\n",
    "\n",
    "# selecting rows n columns\n",
    "# df.loc[row_selection, column_selection]\n",
    "df.loc[['Dean', 'Cornelia'], ['age', 'state', 'score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **.iloc**\n",
    "\n",
    "only uses integer locations to make its selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:43:58.341828Z",
     "start_time": "2019-10-21T00:43:58.282025Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[[5, 2, 4]]\n",
    "\n",
    "df.iloc[3:5]\n",
    "\n",
    "# Select 3rd position to end by 2:\n",
    "df.iloc[3::2]\n",
    "\n",
    "# Select two rows and two columns:\n",
    "df.iloc[[2,3], [0, 4]]\n",
    "\n",
    "# slices for both axes\n",
    "df.iloc[2:5, 2:5]\n",
    "\n",
    "df.iloc[:,2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecting hierarchical columns like the ones returned by grouby\n",
    "\n",
    "**TODO**: How to select multiple rows? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:44:03.032425Z",
     "start_time": "2019-10-21T00:44:02.905517Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example (EXAMPLE NOT IMPLEMENTED YET)\n",
    "df[('loading_time','median')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete colum or row,  reset index,  convert to a type, concat dataframes and sort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Pandas dtype  | Python type | NumPy type                                                     | Usage        |\n",
    "|-------------- | ----------- | -------------------------------------------------------------- | -------------|\n",
    "| object        | str         | string_, unicode_                                              | Text         |\n",
    "| int64         | int         | int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64 | Int,numbers  |\n",
    "| float64       | float       | float_, float16, float32, float64                              | Floa, numbers|\n",
    "| bool          | bool        | bool_                                                          | True/False   |\n",
    "| datetime64    | NA          | datetime64[ns]                                                 | Date n time  |\n",
    "| timedelta[ns] | NA          | NA                                                             | Dif btw times|\n",
    "| category      | NA          | NA                                                             | factors      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:45:06.650047Z",
     "start_time": "2019-10-21T00:45:06.618537Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df = df.drop(columns=['B', 'C'])\n",
    "\n",
    "#drop rows by index\n",
    "df = df.drop([0, 1])\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index()\n",
    "\n",
    "# see the types\n",
    "df.dtypes\n",
    "\n",
    "# see more info <= BETTER THAN dtypes for inspection\n",
    "df.info()\n",
    "\n",
    "# convert to a type\n",
    "df['Customer Number'] = df['Customer Number'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T00:45:13.328789Z",
     "start_time": "2019-10-21T00:45:13.317658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ex: using concat (Ex not implemented yet)\n",
    "pages_view = pd.concat(pages, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex: sort\n",
    "df.sort_values(['a', 'b'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Work with category (factors) data\n",
    "\n",
    "https://pbpython.com/categorical-encoding.html\n",
    "\n",
    "* How to convert to category\n",
    "* Label encoding\n",
    "* One hot encode\n",
    "*\n",
    "\n",
    "\n",
    "Only if I see values\n",
    "See this article http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/\n",
    "and the package is here http://contrib.scikit-learn.org/categorical-encoding/\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T04:46:54.534947Z",
     "start_time": "2019-03-08T04:46:44.034255Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bikes = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "bikes.columns = ['West', 'East']\n",
    "\n",
    "bikes.shape\n",
    "\n",
    "bikes = bikes.dropna()\n",
    "bikes.shape\n",
    "\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T22:55:55.100277Z",
     "start_time": "2019-04-08T22:55:55.053383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = {'col_1': [3, 2, 3, 0], 'col_2': ['a', 'b', 'a', 'd']}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head()\n",
    "\n",
    "# last or first <= default or False = drop all cases\n",
    "# subset to ignore olther columns while considering duplicated\n",
    "df = df.drop_duplicates(keep='last',subset=['col_1','col_2'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T07:31:05.742891Z",
     "start_time": "2018-12-03T07:31:05.736887Z"
    }
   },
   "source": [
    "### Path manipulations\n",
    "\n",
    "http://pbpython.com/pathlib-intro.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pathlib_cheatsheet_p1.png\" width=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T04:08:42.655092Z",
     "start_time": "2019-07-19T04:08:42.626829Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "in_file_1 = Path.cwd() / \"in\" / \"input.xlsx\"\n",
    "\n",
    "print('file: {}'.format(in_file_1))\n",
    "print('basename : {}'.format(in_file_1.name))\n",
    "print('stem : {}'.format(in_file_1.name))\n",
    "print('.ext : {}'.format(in_file_1.suffix))\n",
    "print('dirname : {}'.format(in_file_1.parent))\n",
    "print('up : {}'.format(in_file_1.parent.stem))\n",
    "\n",
    "\n",
    "p = Path.cwd()\n",
    "\n",
    "print('logical checks: is a dir , is a file?')\n",
    "p.is_dir()\n",
    "\n",
    "p.is_file()\n",
    "\n",
    "\n",
    "print('Get parts (split)')\n",
    "p.parts\n",
    "\n",
    "\n",
    "print('paths, parents ...')\n",
    "p.absolute()\n",
    "\n",
    "p.parent\n",
    "\n",
    "p.as_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String manipulation \n",
    "\n",
    "\n",
    "* better way to work with regex\n",
    "* basic string manipulation\n",
    "* vectorized string manipulation\n",
    "  (take notes of the pros n crons)\n",
    "    \n",
    "    https://jakevdp.github.io/PythonDataScienceHandbook/03.10-working-with-strings.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tanagara da Serra'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'95014'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "address = 'Hello, Tanagara da Serra 95014'\n",
    "\n",
    "city_zip_code_regex = r'.*,\\s*(?P<city>.+?)\\s*(?P<zip_code>\\d{5})?$'\n",
    "matches = re.match(city_zip_code_regex, address)\n",
    "\n",
    "matches['city'] \n",
    "matches['zip_code']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply, map and lambda function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/phone_data.csv', index_col=False)\n",
    "data.shape\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dur_min'] = data.duration.map(lambda d: d/60.00)\n",
    "\n",
    "data['concat'] = data.apply(lambda r: r['network'] + '-' + r['item'], axis=1)\n",
    "\n",
    "# apply with args\n",
    "def _foo(row,const_str):\n",
    "    net = row['network']\n",
    "    return net + '-' +  const_str\n",
    "    \n",
    "data['concat'] = data.apply(_foo, args=('const',), axis=1)\n",
    "\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data summarizations and merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Summarize data\n",
    "\n",
    "\n",
    "* descriptive (tables)\n",
    "* groupby\n",
    "* count frequency \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T17:49:20.133229Z",
     "start_time": "2019-09-20T17:49:19.780490Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(830, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>duration</th>\n",
       "      <th>item</th>\n",
       "      <th>month</th>\n",
       "      <th>network</th>\n",
       "      <th>network_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15/10/14 06:58</td>\n",
       "      <td>34.429</td>\n",
       "      <td>data</td>\n",
       "      <td>2014-11</td>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15/10/14 06:58</td>\n",
       "      <td>13.000</td>\n",
       "      <td>call</td>\n",
       "      <td>2014-11</td>\n",
       "      <td>Vodafone</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15/10/14 14:46</td>\n",
       "      <td>23.000</td>\n",
       "      <td>call</td>\n",
       "      <td>2014-11</td>\n",
       "      <td>Meteor</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15/10/14 14:48</td>\n",
       "      <td>4.000</td>\n",
       "      <td>call</td>\n",
       "      <td>2014-11</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>15/10/14 17:27</td>\n",
       "      <td>4.000</td>\n",
       "      <td>call</td>\n",
       "      <td>2014-11</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            date  duration  item    month   network network_type\n",
       "0      0  15/10/14 06:58    34.429  data  2014-11      data         data\n",
       "1      1  15/10/14 06:58    13.000  call  2014-11  Vodafone       mobile\n",
       "2      2  15/10/14 14:46    23.000  call  2014-11    Meteor       mobile\n",
       "3      3  15/10/14 14:48     4.000  call  2014-11     Tesco       mobile\n",
       "4      4  15/10/14 17:27     4.000  call  2014-11     Tesco       mobile"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/phone_data.csv')\n",
    "\n",
    "data.shape\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Count frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T17:49:22.463871Z",
     "start_time": "2019-09-20T17:49:22.450868Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.item.unique()\n",
    "print()\n",
    "data.item.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Groupby\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "Split-Apply-Combine\n",
    "\n",
    "<img src=\"images/split-apply-combine.png\" width=\"400\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T17:53:30.569772Z",
     "start_time": "2019-09-20T17:53:30.548289Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# One option is to compute the group by and compute aggregation laetr as you code\n",
    "# this approachh looks betetr in data wrangling and for plot the results\n",
    "g = data.groupby(['month', 'item'])\n",
    "\n",
    "# group.size is convenient\n",
    "g.size()\n",
    "print()\n",
    "g.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T17:52:55.355131Z",
     "start_time": "2019-09-20T17:52:55.111839Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter(g.duration.mean(),g.size(),edgecolor = 'none',alpha = 0.2, s=20, c='b')\n",
    "plt.xlabel('Group mean relative date')\n",
    "plt.ylabel('Group size')\n",
    "plt.title('Train');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T22:53:33.930715Z",
     "start_time": "2019-06-26T22:53:33.851129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Group the data frame by month and item and extract a number of stats from each group\n",
    "data.groupby(['month', 'item'], as_index=False).aggregate({\n",
    "                                 'duration': {'min': min, '1q': lambda x: np.percentile(x,25),'median': np.median,\n",
    "                                                          '3q': lambda x: np.percentile(x,75), 'max': max,\n",
    "                                                          'std': np.std, 'cv': lambda x: np.median(x)/np.std(x)},\n",
    "                                 'date':  'first',       # get the first date per group\n",
    "                                 'network_type': ['nunique', 'count']\n",
    "                                    }).sort_values(by=[('duration','min')],ascending=False, na_position='last').head(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T06:31:58.058543Z",
     "start_time": "2018-12-03T06:31:57.994453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(data.network_type, data.item, margins=True, margins_name=\"Total\")\n",
    "\n",
    "pd.crosstab(data.network_type, data.item, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**comparing with groub by and pivot table**\n",
    "\n",
    "crosstabe is more easy to read and do not have the extr steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T06:36:33.987575Z",
     "start_time": "2018-12-03T06:36:33.936219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(data.network_type, data.item)\n",
    "\n",
    "data.groupby(['network_type', 'item'])['network_type'].count().unstack().fillna(0)\n",
    "\n",
    "data.pivot_table(index='item', columns='network_type', aggfunc={'network_type':len}, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T06:42:36.398408Z",
     "start_time": "2018-12-03T06:42:36.392896Z"
    },
    "hidden": true
   },
   "source": [
    "#### pivot table\n",
    "\n",
    "http://pbpython.com/pandas-pivot-table-explained.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/pivot-table-datasheet.png\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T06:49:40.134135Z",
     "start_time": "2018-12-03T06:49:40.091664Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = pd.pivot_table(data,index=[\"item\",\"network_type\"],\n",
    "               values=[\"duration\"],\n",
    "               aggfunc=[np.sum,np.mean],fill_value=0,margins=True)\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T06:56:33.823133Z",
     "start_time": "2018-12-03T06:56:33.805852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "table.query('network_type == [\"data\", \"sms\", \"mobile\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Joins\n",
    "\n",
    "\n",
    "left , right n inner join\n",
    "\n",
    "| Merge method | SQL Join Name    | Description                               |  \n",
    "|--------------|----------------- |-------------------------------------------|  \n",
    "| left         | LEFT OUTER JOIN  | Use keys from left frame only             |  \n",
    "| right        | RIGHT OUTER JOIN | Use keys from right frame only            |  \n",
    "| outer        | FULL OUTER JOIN  | Use union of keys from both frames        |  \n",
    "| inner        | INNER JOIN       | Use intersection of keys from both frames |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T22:43:45.755883Z",
     "start_time": "2018-11-19T22:43:45.710753Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "                     'key2': ['K0', 'K1', 'K0', 'K1'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3'], \n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "\n",
    "\n",
    "right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "                      'key2': ['K0', 'K0', 'K0', 'K0'],  \n",
    "                      'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                      'D': ['D0', 'D1', 'D2', 'D3']})\n",
    "\n",
    "result = pd.merge(left, right, how='left', on=['key1', 'key2'])\n",
    "\n",
    "left\n",
    "\n",
    "right\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Cartesian product or cross join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T17:55:51.186981Z",
     "start_time": "2019-04-26T17:55:51.165505Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key1': ['K0', 'K1', 'K2', 'K3'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3']})\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame({'key1': ['Q0', 'Q1', 'Q2', 'Q3'],\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "\n",
    "\n",
    "# Add dummy key\n",
    "df1['_tmpkey'] = 1\n",
    "df2['_tmpkey'] = 1\n",
    "\n",
    "cartesian_df = pd.merge(df1[['_tmpkey',\"A\"]], df2[['_tmpkey',\"B\"]], on='_tmpkey').drop('_tmpkey', axis=1)\n",
    "#cartesian_df.index = pd.MultiIndex.from_product((df1.index, df2.index))\n",
    "cartesian_df = cartesian_df.reset_index(drop=True)\n",
    "\n",
    "df1.drop('_tmpkey', axis=1, inplace=True)\n",
    "df2.drop('_tmpkey', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "cartesian_df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### High performance eval n query\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/03.12-performance-eval-and-query.html\n",
    "\n",
    "Motivations\n",
    "for large df n arrays eval n query are faster\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
