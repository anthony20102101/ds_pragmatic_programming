{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark (WIP)\n",
    "\n",
    "\n",
    "**TODO: Merging with ds_pragmatic_programming_pyspark**\n",
    "\n",
    "* how to run using docker image for spark\n",
    "\n",
    "    * options is the course docker image: ucsddse230/cse255-dse230\n",
    " \n",
    "```sh\n",
    "docker run --name edx_big_data -it -p 8889:8888 -v /media/leandroohf/sdb1/leandro/edx_big_data_analytics_using_spark:/home/ucsddse230/ ucsddse230/cse255-dse230 /bin/bash\n",
    "\n",
    "# If you need to ssh to the container\n",
    "docker exec -it edx_big_data /bin/bash\n",
    "\n",
    "```\n",
    "    * search a simple docker image in docker hub\n",
    "\n",
    "\n",
    "* refs:\n",
    "\n",
    "    * https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/course/\n",
    "    * https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/fe9a95cc542d4c30b855e632663c4797/8?activate_block_id=block-v1%3ABerkeleyX%2BCS105x%2B1T2016%2Btype%40vertical%2Bblock%4083ff2d3b4e93489b9b7b4861811e0872\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T20:31:46.676642Z",
     "start_time": "2019-10-21T20:31:45.640583Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "%pylab inline\n",
    "\n",
    "#start the SparkContext\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T20:30:20.515980Z",
     "start_time": "2019-10-21T20:30:20.285848Z"
    }
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls data/Weather/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T20:31:18.769016Z",
     "start_time": "2019-10-21T20:31:18.230659Z"
    }
   },
   "outputs": [],
   "source": [
    "# form txt file \n",
    "!head data/Moby-Dick.txt\n",
    "\n",
    "text_file = sc.textFile('data/Moby-Dick.txt')\n",
    "type(text_file)\n",
    "\n",
    "\n",
    "pair_rdd = sc.parallelize([(1,2), (3,4)])\n",
    "print(pair_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfortmations\n",
    "\n",
    "* You cannot use any operation on the map functio. The operation should NOT depend of the other like subtraction or division. Will get different results while runnning multiple times\n",
    "\n",
    " Transformations on (key,value) rdds. **RDD $\\to$ RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map, filter n sample \n",
    "\n",
    "* **No** communication needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_rdd = sc.parallelize([1, 2, 3, 4, 2, 5, 6])\n",
    "\n",
    "pair_rdd = regular_rdd.map( lambda x: (x, x*x) )\n",
    "print(pair_rdd.collect())\n",
    "\n",
    "print(regular_rdd.filter( lambda x: x > 3 ).collect())\n",
    "\n",
    "# sample(withReplacement, fraction, seed)\n",
    "print(regular_rdd.sample(True, 0.5, 11))\n",
    "\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "\n",
    "# LHOF Notes\n",
    "x = 3\n",
    "print('list: ', list(range(x,x+2)))\n",
    "\n",
    "# the lambda function generates for each number i, an iterator that produces i,i+1\n",
    "print(\"After transformation : \", rdd.flatMapValues(lambda x: list(range(x,x+2))).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupbyKey n reduceByKey\n",
    "\n",
    "**Shuffles:** RDD $\\to$ RDD, **shuffle** needed\n",
    "\n",
    "**Shuffles are costly transfromations**\n",
    "\n",
    "* **Examples:** sort, distinct, repartition, sortByKey, reduceByKey, join [More](http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations)\n",
    "  * **A LOT** of communication might be needed.\n",
    "\n",
    "\n",
    "**Properties of reduce operations**\n",
    "\n",
    "* Reduce operations **must not depend on the order**\n",
    "  * Order of operands should not matter\n",
    "  * Order of application of reduce operator should not matter\n",
    "\n",
    "* Multiplication and summation are good:\n",
    "\n",
    "```\n",
    "                1 + 3 + 5 + 2                      5 + 3 + 1 + 2 \n",
    "```\n",
    "\n",
    " * Division and subtraction are bad:\n",
    "\n",
    "```\n",
    "                    1 - 3 - 5 - 2                      1 - 3 - 5 - 2\n",
    "```\n",
    "\n",
    "\n",
    "**groupByKey():**\n",
    "Returns a new RDD of `(key,<iterator>)` pairs where the iterator iterates over the values associated with the key.\n",
    "\n",
    "\n",
    "[Iterators](http://anandology.com/python-practice-book/iterators.html) are python objects that generate a sequence of values. Writing a loop over `n` elements as \n",
    "```python\n",
    "for i in range(n):\n",
    "    ##do something\n",
    "```\n",
    "is inefficient because it first allocates a list of `n` elements and then iterates over it.\n",
    "Using the iterator `xrange(n)` achieves the same result without materializing the list. Instead, elements are generated on the fly.\n",
    "\n",
    "To materialize the list of values returned by an iterator we will use the list comprehension command:\n",
    "```python\n",
    "[a for a in <iterator>]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey return (key, <iterator>)\n",
    "\n",
    "A = sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])\n",
    "A.groupByKey().mapValues(lambda x: [elem for elem in x ])\n",
    "\n",
    "# output\n",
    "#[ (1, [3,-5]), (3, [100, 2]) ]\n",
    "\n",
    "print(A.groupByKey().map(lambda elem: (elem[0],[x for x in elem[1] ])).collect())\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "print(\"After transformation : \", rdd.reduceByKey(lambda a,b: a+b).collect())\n",
    "\n",
    "\n",
    "rdd = sc.parallelize([(2,2), (1,4), (3,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "print(\"After transformation : \", rdd.sortByKey().collect())\n",
    "\n",
    "# Using sortBy\n",
    "print(\"After transformation : \", rdd.sortBy(lambda x: x[1],ascending=False).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations 2 rdds\n",
    "\n",
    "**subtractByKey**\n",
    "Remove from RDD1 all elements whose key is present in RDD2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LHOF Notes\n",
    "\n",
    "rdd1 = sc.parallelize([(1,2), (2,1), (2,2)])\n",
    "rdd2 = sc.parallelize([(2,5), (3,1)])\n",
    "\n",
    "print('rdd1: ', rdd1.collect())\n",
    "print('rdd2: ', rdd2.collect())\n",
    "print('subtractByKey: ', rdd1.subtractByKey(rdd2).collect())\n",
    "\n",
    "print()\n",
    "# Pay attention. This is a set operation\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "\n",
    "print('x: ', x.collect())\n",
    "print('y: ', y.collect())\n",
    "print('subtract: ', sorted(x.subtract(y).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**join**\n",
    "\n",
    "* A fundamental operation in relational databases.\n",
    "* assumes two tables have a **key** column in common. \n",
    "* merges rows with the same key.\n",
    "\n",
    "\n",
    "When `Join` is called on datasets of type `(Key, V)` and `(Key, W)`, it  returns a dataset of `(Key, (V, W))` pairs with all pairs of elements for each key. Joining the 2 datasets above yields: \n",
    "\n",
    "\n",
    "There are four variants of `join` which differ in how they treat keys that appear in one dataset but not the other.\n",
    "* `join` is an *inner* join which means that keys that appear only in one dataset are eliminated.\n",
    "* `leftOuterJoin` keeps all keys from the left dataset even if they don't appear in the right dataset. The result of leftOuterJoin in our example will contain the keys `John, Jill, Kate`\n",
    "* `rightOuterJoin` keeps all keys from the right dataset even if they don't appear in the left dataset. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John`\n",
    "* `FullOuterJoin` keeps all keys from both datasets. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John, Kate`\n",
    "\n",
    "In outer joins, if the element appears only in one dataset, the element in `(K,(V,W))` that does not appear in the dataset is represented bye `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OuterJoin\n",
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())\n",
    "print(\"Result:\", rdd1.rightOuterJoin(rdd2).collect())\n",
    "\n",
    "print()\n",
    "\n",
    "# leftOuterJoin\n",
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())\n",
    "print(\"Result:\", rdd1.leftOuterJoin(rdd2).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "Actions on (key,val) RDDs. **RDD $\\to$ Python-object in head node.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  countByKey: returns dictionary\n",
    "A = sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])\n",
    "\n",
    "A.countByKey()\n",
    "\n",
    "# output (dictionnary\n",
    "# {1:2, 3:2}\n",
    "\n",
    "# lookup (key): returns the list of all of the values associated with key\n",
    "A = sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])\n",
    "\n",
    "A.lookup(3)\n",
    "\n",
    "# output (list)\n",
    "# [100,2]\n",
    "\n",
    "#  collectAsMap(): like collect() - collect returns list of tuples -  but returns a map = Dictionary\n",
    "A = sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])\n",
    "A.collectAsMap()\n",
    "\n",
    "# output Dictionary\n",
    "# {1:[3,-5], 3: [100,2]}\n",
    "\n",
    "regular_rdd = sc.parallelize([1, 2, 3, 4, 2, 5, 6])\n",
    "\n",
    "# takeSample(withReplacement, num, [seed])\n",
    "print(regular_rdd.sample(True, 5, 11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Famous word count example (hello word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Famous word count example (hello word)\n",
    "\n",
    "words = ['this', 'is', 'the', 'best', 'mac', 'ever']\n",
    "\n",
    "wordRDD = sc.parallelize(words)\n",
    "\n",
    "wordRDD.reduce(lambda w,v: w if len(w) < len(v) else v)\n",
    "\n",
    "# another example is sum (the prder does not matter)\n",
    "B=sc.parallelize([1,3,5,2])\n",
    "\n",
    "B.reduce(lambda x,y: x+y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and write from disk\n",
    "\n",
    "\n",
    "* parquet files (it is folder) are very popular e efficeent for IO in disk\n",
    "* parqet can be query directly from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
